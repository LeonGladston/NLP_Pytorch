{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_generation_using_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dca40b1816954404bc07113e0811f314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_38a506d44be144f88137ff8a581a3955",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e7fa5547bfe54896b40693f67e2f4ae0",
              "IPY_MODEL_4d58b56ada1944d98e6d493efc98edf3"
            ]
          }
        },
        "38a506d44be144f88137ff8a581a3955": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7fa5547bfe54896b40693f67e2f4ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6cd0f817bf5c4990abe877300d0e35f0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 64776,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 64776,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d99e81b3403146488010d967b3944462"
          }
        },
        "4d58b56ada1944d98e6d493efc98edf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_905d1da7a8b8443ab4241b2e4ab293c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 64776/64776 [00:31&lt;00:00, 2061.20it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f5e450f35c8a4ab4a79ad48e12654b4e"
          }
        },
        "6cd0f817bf5c4990abe877300d0e35f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d99e81b3403146488010d967b3944462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "905d1da7a8b8443ab4241b2e4ab293c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f5e450f35c8a4ab4a79ad48e12654b4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPP9M9c64Tp-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XvJfuJTAupL"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import random\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIWCKSYuBfje",
        "outputId": "f7ebf9df-9113-4b91-f816-66ef17878044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# reproducing same results\n",
        "SEED = 2019\n",
        "\n",
        "# torch\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f17fa00b360>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScS2N8eeDO7x",
        "outputId": "e725ed53-a5dd-4430-8e84-f659a1a5b5b5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2698c34c-fe13-47cf-b4b9-60259a12a849\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2698c34c-fe13-47cf-b4b9-60259a12a849\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Dailog-dataset.dialogs_dataset to Dailog-dataset.dialogs_dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SbaiSkgBprV"
      },
      "source": [
        "# open text file and read in data\n",
        "with open(\"Dailog-dataset.dialogs_dataset\", \"rb\") as f:\n",
        "  dialogs = pickle.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQPNChNFBps1"
      },
      "source": [
        "# text cleaning\n",
        "dialogs_clean = []\n",
        "\n",
        "for i in dialogs:\n",
        "  # remove everything except alphabets\n",
        "  i = re.sub(\"[^a-zA-Z' ]\", \"\", i)\n",
        "  # convert text to lowercase\n",
        "  i = i.lower()\n",
        "  # add cleaned text to the list\n",
        "  dialogs_clean.append(i)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5J_2_0cBpw-"
      },
      "source": [
        "# get list of all the words\n",
        "all_words = \" \".join(dialogs_clean).split()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtMO0n9dIkRL"
      },
      "source": [
        "words_dict = {}\n",
        "\n",
        "# add word-count pair to the dictionary\n",
        "for word in all_words:   \n",
        "  # check if the word is already in dictionary \n",
        "  if word in words_dict:\n",
        "    # increment count of word by 1 \n",
        "    words_dict[word] = words_dict[word] + 1\n",
        "  else:\n",
        "    # add the word to dictionary with count 1 \n",
        "    words_dict[word] = 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muZfSdjLIkTe",
        "outputId": "5ccbd7dd-a49e-40c8-f339-c79be96de936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# to check\n",
        "total_values = words_dict.values()\n",
        "print(sum(total_values))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "475893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aLG6hYdIkV2"
      },
      "source": [
        "# prepare a dataframe\n",
        "words_df = pd.DataFrame({'word':list(words_dict.keys()), 'count':list(words_dict.values())})\n",
        "\n",
        "# sort words by their count in increasing order\n",
        "words_df = words_df.sort_values(by = ['count'])\n",
        "\n",
        "# reset dataframe index\n",
        "words_df.reset_index(inplace = True, drop=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEWidw34I7t3"
      },
      "source": [
        "Find and Replace Rare Words with \"Unknown\" Token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOxlN5rFIsNH"
      },
      "source": [
        " #user specified threshold value\n",
        "rare_thresh = 4\n",
        "\n",
        "# get percentage of rare words in the vocabulary\n",
        "rare_words_count = len(words_df[words_df['count'] < rare_thresh]['word'])\n",
        "total_words = len(words_df) \n",
        "rare_dist = rare_words_count / total_words\n",
        "\n",
        "# coverage percentage of rare words in the corpus\n",
        "rare_cover = words_df[words_df['count'] < rare_thresh]['count'].sum()/words_df['count'].sum()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPJJNSyfIsPg",
        "outputId": "b73c4434-ab87-4aa5-f82d-f79a3159c68f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(f\"Rare words distribution in the vocabulary: {rare_dist*100:.2f}\")\n",
        "print(f\"Rare words coverage in the corpus: {rare_cover*100:.2f}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rare words distribution in the vocabulary: 69.03\n",
            "Rare words coverage in the corpus: 2.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R5VKkBhJyLW"
      },
      "source": [
        "# extract rare words in a list\n",
        "rare_words = words_df[words_df['count'] < rare_thresh]['word'].tolist()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "751zGu59JfZj"
      },
      "source": [
        "Let's see the technique that we will use to replace the rare words/tokens in the dataset with a special token known as the unknown token (\"<unk>\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSfnJGLcIsRh",
        "outputId": "82a99100-1fc3-4cfb-f50e-033d88913c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "dca40b1816954404bc07113e0811f314",
            "38a506d44be144f88137ff8a581a3955",
            "e7fa5547bfe54896b40693f67e2f4ae0",
            "4d58b56ada1944d98e6d493efc98edf3",
            "6cd0f817bf5c4990abe877300d0e35f0",
            "d99e81b3403146488010d967b3944462",
            "905d1da7a8b8443ab4241b2e4ab293c7",
            "f5e450f35c8a4ab4a79ad48e12654b4e"
          ]
        }
      },
      "source": [
        "# create a text pattern from the rare words, like \"word1 | word2 | word3...\"\n",
        "pattern = \"\"\n",
        "for i in rare_words:\n",
        "  pattern+= \" {} |\".format(i)\n",
        "\n",
        "# removing the last element which is \"|\"\n",
        "pattern = pattern[:-1]\n",
        "\n",
        "# empty list \n",
        "dialogs_clean_v2 = []\n",
        "\n",
        "# replace the rare words with the <unk> token\n",
        "for d in tqdm_notebook(dialogs_clean):\n",
        "  text = re.sub(pattern, \" <unk> \", d)\n",
        "  dialogs_clean_v2.append(text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dca40b1816954404bc07113e0811f314",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=64776.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDSv96G7IsTl",
        "outputId": "03cfcce3-42a7-4f1e-abdd-293d95e0c1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "dialogs_clean_v2[520:530]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['does it serve traditional chinese dessert',\n",
              " 'how much extra time to reach <unk> ',\n",
              " 'ok lets reserve a table for dinner at hakkasan',\n",
              " 'hello i need to get a car please',\n",
              " 'holiday inn <unk> parkconv <unk> convention center drive <unk> park il',\n",
              " 'bowling alley <unk> highway <unk> park il',\n",
              " 'what types of cars does uber have',\n",
              " \"what's the price difference\",\n",
              " 'ok get me the cheapest please',\n",
              " 'ok then get me the next level']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkq8s6HCJ_6I"
      },
      "source": [
        "## **Data Preperation**\n",
        "a.Prepare sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqYlmOhVIsaE",
        "outputId": "cc6c2a6a-206f-4fe1-914d-7a20a2fa304d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "# capture length of all the sequences\n",
        "text_word_count = []\n",
        "for i in dialogs_clean_v2:\n",
        "  text_word_count.append(len(i.split()))\n",
        "        \n",
        "# plot the sequence lengths\n",
        "pd.Series(text_word_count).hist(bins = 30,range=(0,30))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f17acc9a828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhL62Tq7IsX9"
      },
      "source": [
        " #function to create sequences of equal length\n",
        "def create_seq(text, seq_len = 5):\n",
        "      \n",
        "  sequences = []    \n",
        "  \n",
        "  if len(text.split()) > seq_len:\n",
        "    for i in range(seq_len, len(text.split())):\n",
        "      # select sequence of tokens\n",
        "      seq = text.split()[i-seq_len:i+1]\n",
        "      # append sequence to the list\n",
        "      sequences.append(\" \".join(seq))\n",
        "\n",
        "    return sequences\n",
        "\n",
        "  else:\n",
        "    \n",
        "    return [text]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPJWV7slIsWX"
      },
      "source": [
        "# create sequences of equal length\n",
        "seqs = [create_seq(i) for i in dialogs_clean_v2]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u6unZRMIkYe",
        "outputId": "a58c1ff7-137c-48b4-a1e2-7856afadd3e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        }
      },
      "source": [
        "seqs[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[\"hi i'm looking to book a\",\n",
              "  \"i'm looking to book a table\",\n",
              "  'looking to book a table for',\n",
              "  'to book a table for korean',\n",
              "  'book a table for korean fod'],\n",
              " ['somewhere in southern nyc maybe the',\n",
              "  'in southern nyc maybe the east',\n",
              "  'southern nyc maybe the east village'],\n",
              " [\"we don't want to sit at\",\n",
              "  \"don't want to sit at the\",\n",
              "  'want to sit at the bar',\n",
              "  'to sit at the bar but',\n",
              "  'sit at the bar but anywhere',\n",
              "  'at the bar but anywhere else',\n",
              "  'the bar but anywhere else is',\n",
              "  'bar but anywhere else is fine'],\n",
              " ['what times are available'],\n",
              " [\"yikes we can't do those times\"],\n",
              " ['let me check'],\n",
              " [\"great let's book that\"],\n",
              " [\"no that's it just book\"],\n",
              " ['hi i would like to see',\n",
              "  'i would like to see if',\n",
              "  'would like to see if the',\n",
              "  'like to see if the movie',\n",
              "  'to see if the movie what',\n",
              "  'see if the movie what men',\n",
              "  'if the movie what men want',\n",
              "  'the movie what men want is',\n",
              "  'movie what men want is playing',\n",
              "  'what men want is playing here'],\n",
              " ['yes for me and a friend',\n",
              "  'for me and a friend so',\n",
              "  'me and a friend so two',\n",
              "  'and a friend so two tickets',\n",
              "  'a friend so two tickets please']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duwMSjXLIkbE"
      },
      "source": [
        "# merge list-of-lists into a single list\n",
        "seqs = sum(seqs, [])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoYzmBqzKcnq",
        "outputId": "cc6ec3a9-6c6a-4616-eaf2-651741590378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "seqs[:15]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"hi i'm looking to book a\",\n",
              " \"i'm looking to book a table\",\n",
              " 'looking to book a table for',\n",
              " 'to book a table for korean',\n",
              " 'book a table for korean fod',\n",
              " 'somewhere in southern nyc maybe the',\n",
              " 'in southern nyc maybe the east',\n",
              " 'southern nyc maybe the east village',\n",
              " \"we don't want to sit at\",\n",
              " \"don't want to sit at the\",\n",
              " 'want to sit at the bar',\n",
              " 'to sit at the bar but',\n",
              " 'sit at the bar but anywhere',\n",
              " 'at the bar but anywhere else',\n",
              " 'the bar but anywhere else is']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoSvriFmKcqE",
        "outputId": "12bacd2d-72ee-40b3-a9ca-c89fe888ba77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# count of sequences\n",
        "len(seqs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGKM0gYOKctN"
      },
      "source": [
        "# create input and target sequences (x and y)\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for s in seqs:\n",
        "  x.append(\" \".join(s.split()[:-1]))\n",
        "  y.append(\" \".join(s.split()[1:]))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XpRJYMlKcvT",
        "outputId": "fe2f62b2-ee27-4d04-b7da-2ee4e108a5de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "x[0], y[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"hi i'm looking to book\", \"i'm looking to book a\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXa723C4tLl-",
        "outputId": "1fd70daf-7b49-422e-e5c7-b36ce5e3d8ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "x[88543], y[88543]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('to drive to several locations', 'drive to several locations do')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpOqibdBtg7f"
      },
      "source": [
        "## 4.2 Create Token-Integer Mappings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8FqVsNktLr5"
      },
      "source": [
        "# create integer-to-token mapping\n",
        "int2token = {}\n",
        "cnt = 1\n",
        "\n",
        "for w in set(\" \".join(dialogs_clean_v2).split()):\n",
        "  int2token[cnt] = w\n",
        "  cnt+= 1\n",
        "\n",
        "# create token-to-integer mapping\n",
        "token2int = {t: i for i, t in int2token.items()}"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1gcEBoJtLv_",
        "outputId": "9092db92-8901-4c72-a0cf-3d600556e04c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "token2int[\"can\"], int2token[1127]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4421, 'marley')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4EsqLZyt03i"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJFedvD7t3Cv"
      },
      "source": [
        "## 4.3 Split Data into Train and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7ImIplqtL3T"
      },
      "source": [
        "# train-validation split\n",
        "# input sequences\n",
        "x_tr = x[:150000]\n",
        "x_val = x[150000:]\n",
        "\n",
        "# target sequences\n",
        "y_tr = y[:150000]\n",
        "y_val = y[150000:]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDv9yh2ft_fB"
      },
      "source": [
        "## 4.4 Pad Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRr1pSCjtL44",
        "outputId": "68f7a6eb-e924-4125-9a19-df0a430a7a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "# plot sequence length in train set\n",
        "text_word_count = []\n",
        "\n",
        "for i in x_tr:\n",
        "  text_word_count.append(len(i.split()))\n",
        "\n",
        "pd.Series(text_word_count).hist(bins = 70,range=(0,30))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f17a9a664a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-yCF9HWtMCA"
      },
      "source": [
        "# based on the plot above\n",
        "max_text_len = 5\n",
        "\n",
        "# function to perform padding\n",
        "def pad_sequence(seq, n):\n",
        "\n",
        "  # split input sequence into tokens\n",
        "  seq = seq.split()\n",
        "  \n",
        "  # check if no. of tokens in input sequence is less than 'n'\n",
        "  if len(seq) < n:\n",
        "    for i in range(n - len(seq)):\n",
        "      seq.append(\"<pad>\")\n",
        "\n",
        "  return \" \".join(seq)\n",
        "\n",
        "# pad text sequences (train set)\n",
        "x_tr_padded = [pad_sequence(s, max_text_len) for s in x_tr]\n",
        "y_tr_padded = [pad_sequence(s, max_text_len) for s in y_tr]\n",
        "\n",
        "# pad text sequences (validation set)\n",
        "x_val_padded = [pad_sequence(s, max_text_len) for s in x_val]\n",
        "y_val_padded = [pad_sequence(s, max_text_len) for s in y_val]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abZJ_zHStMF8",
        "outputId": "7d206cc2-ca0a-4a3b-8f6b-36ed1ab3a299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "x_tr_padded[:20]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"hi i'm looking to book\",\n",
              " \"i'm looking to book a\",\n",
              " 'looking to book a table',\n",
              " 'to book a table for',\n",
              " 'book a table for korean',\n",
              " 'somewhere in southern nyc maybe',\n",
              " 'in southern nyc maybe the',\n",
              " 'southern nyc maybe the east',\n",
              " \"we don't want to sit\",\n",
              " \"don't want to sit at\",\n",
              " 'want to sit at the',\n",
              " 'to sit at the bar',\n",
              " 'sit at the bar but',\n",
              " 'at the bar but anywhere',\n",
              " 'the bar but anywhere else',\n",
              " 'bar but anywhere else is',\n",
              " 'what times are <pad> <pad>',\n",
              " \"yikes we can't do those\",\n",
              " 'let me <pad> <pad> <pad>',\n",
              " \"great let's book <pad> <pad>\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWVniEKytMK3",
        "outputId": "b84a9ac3-74cf-413c-adc5-33606adb5ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "y_tr_padded[:20]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"i'm looking to book a\",\n",
              " 'looking to book a table',\n",
              " 'to book a table for',\n",
              " 'book a table for korean',\n",
              " 'a table for korean fod',\n",
              " 'in southern nyc maybe the',\n",
              " 'southern nyc maybe the east',\n",
              " 'nyc maybe the east village',\n",
              " \"don't want to sit at\",\n",
              " 'want to sit at the',\n",
              " 'to sit at the bar',\n",
              " 'sit at the bar but',\n",
              " 'at the bar but anywhere',\n",
              " 'the bar but anywhere else',\n",
              " 'bar but anywhere else is',\n",
              " 'but anywhere else is fine',\n",
              " 'times are available <pad> <pad>',\n",
              " \"we can't do those times\",\n",
              " 'me check <pad> <pad> <pad>',\n",
              " \"let's book that <pad> <pad>\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX6XfZVhtMQC"
      },
      "source": [
        "# update mapping dictionaries\n",
        "int2token[0] = \"<pad>\"\n",
        "token2int[\"<pad>\"] = 0\n",
        "\n",
        "# set vocabulary size\n",
        "vocab_size = len(int2token)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hfkL-dNtMTi",
        "outputId": "43d4fa81-b4ef-4d47-a18b-cd1655531b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "len(int2token)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6502"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL0nwTf3vFJv"
      },
      "source": [
        "## 4.5 Convert Text Sequences to Integer Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca7G6P-3tMOF"
      },
      "source": [
        "# function to create integer sequences\n",
        "def get_integer_seq(seq):\n",
        "  return [token2int[w] for w in seq.split()]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XycM3ML0tMJF"
      },
      "source": [
        "# convert text sequences to integer sequences\n",
        "x_tr_int = [get_integer_seq(i) for i in x_tr_padded]\n",
        "y_tr_int = [get_integer_seq(i) for i in y_tr_padded]\n",
        "\n",
        "x_val_int = [get_integer_seq(i) for i in x_val_padded]\n",
        "y_val_int = [get_integer_seq(i) for i in y_val_padded]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqVzXzhbvV8W"
      },
      "source": [
        "### For Windows users ,need to convert dataset to Long or 64bit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gg98m7ptMEy"
      },
      "source": [
        "# x_tr_int = torch.tensor(x_tr_int).to(torch.int64)\n",
        "# y_tr_int = torch.tensor(y_tr_int).to(torch.int64)\n",
        "\n",
        "# x_val_int = torch.tensor(x_val_int).to(torch.int64)\n",
        "# y_val_int = torch.tensor(y_val_int).to(torch.int64)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xfgLAmctL_4",
        "outputId": "333561b1-3cdf-40e7-98c7-1afaf83a5a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "x_tr_int[:10]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[346, 485, 5465, 394, 5729],\n",
              " [485, 5465, 394, 5729, 5845],\n",
              " [5465, 394, 5729, 5845, 4961],\n",
              " [394, 5729, 5845, 4961, 20],\n",
              " [5729, 5845, 4961, 20, 4067],\n",
              " [1409, 6381, 5658, 4317, 6099],\n",
              " [6381, 5658, 4317, 6099, 5078],\n",
              " [5658, 4317, 6099, 5078, 978],\n",
              " [4364, 4124, 3655, 394, 1330],\n",
              " [4124, 3655, 394, 1330, 5936]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI-dWGnxtL9Y",
        "outputId": "cefed54f-6bbe-4ac0-8a50-f61e5aba5346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "y_tr_int[:10]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[485, 5465, 394, 5729, 5845],\n",
              " [5465, 394, 5729, 5845, 4961],\n",
              " [394, 5729, 5845, 4961, 20],\n",
              " [5729, 5845, 4961, 20, 4067],\n",
              " [5845, 4961, 20, 4067, 5030],\n",
              " [6381, 5658, 4317, 6099, 5078],\n",
              " [5658, 4317, 6099, 5078, 978],\n",
              " [4317, 6099, 5078, 978, 6144],\n",
              " [4124, 3655, 394, 1330, 5936],\n",
              " [3655, 394, 1330, 5936, 5078]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mviIn1sItL8R",
        "outputId": "cc4306f3-e513-4028-ce31-bb23e9581667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# convert lists into numpy arrays\n",
        "x_tr_int = np.array(x_tr_int)\n",
        "y_tr_int = np.array(y_tr_int)\n",
        "\n",
        "x_val_int = np.array(x_val_int)\n",
        "y_val_int = np.array(y_val_int)\n",
        "\n",
        "x_tr_int.shape, y_tr_int.shape, x_val_int.shape, y_val_int.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150000, 5), (150000, 5), (55346, 5), (55346, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT3AFrh8tL0M",
        "outputId": "1052441d-521b-4a53-fe82-f9ff9e79bdef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "x_tr_int[:1]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 346,  485, 5465,  394, 5729]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN3dpHpPwhY5"
      },
      "source": [
        "# 5. Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRcddl6mwmnr"
      },
      "source": [
        "## 5.1 Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EXOaJuLtLy5"
      },
      "source": [
        "## define model architecture\n",
        "\n",
        "## embedding layer: \n",
        "##    input dim = vocab_size, \n",
        "##    ouput dim = 200\n",
        "\n",
        "## LSTM layer:\n",
        "##    input dim = 200\n",
        "##    hidden units = 256\n",
        "##    layers = 2\n",
        "##    output dim = 256\n",
        "\n",
        "## Dropout Layer\n",
        "##    input dim = 256\n",
        "##    output dim = 256\n",
        "\n",
        "## fully connected layer\n",
        "##    input dim = 256\n",
        "##    ouput dim = vocab_size\n",
        "\n",
        "class WordLSTM(nn.Module):\n",
        "      \n",
        "  def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.3, lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "    \n",
        "    self.emb_layer = nn.Embedding(vocab_size, 200)\n",
        "\n",
        "    ## define the LSTM\n",
        "    # input data is of shape (batch size, sequence length, no. of features)...\n",
        "    # ...therefore we need batch_first=True\n",
        "    self.lstm = nn.LSTM(200, n_hidden, n_layers, batch_first=True)\n",
        "    \n",
        "    ## define a dropout layer\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    \n",
        "    ## define the fully-connected layer\n",
        "    self.fc = nn.Linear(n_hidden, vocab_size)      \n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    ''' Forward pass through the network. \n",
        "        These inputs are x, and the hidden/cell state is `hidden`. '''\n",
        "\n",
        "    ## pass input through embedding layer\n",
        "    embedded = self.emb_layer(x)     \n",
        "    \n",
        "    ## Get the outputs and the new hidden state from the lstm\n",
        "    lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "    \n",
        "    ## pass through a dropout layer\n",
        "    out = self.dropout(lstm_output)\n",
        "    \n",
        "    ## reshape the tensor to the shape (batch-size*sequence length, hidden units)\n",
        "    out = out.reshape(-1, self.n_hidden)\n",
        "\n",
        "    ## put \"out\" through the fully-connected layer\n",
        "    out = self.fc(out)\n",
        "\n",
        "    # return the final output and the hidden state\n",
        "    return out, hidden\n",
        "    \n",
        "    \n",
        "  def init_hidden(self, batch_size):\n",
        "    ''' Initializes hidden state '''\n",
        "    # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "    # initialized to zero, for hidden state and cell state of LSTM\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if (torch.cuda.is_available()):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "    \n",
        "    return hidden"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHWmE838wnvQ",
        "outputId": "6afa92ec-352c-4ce4-8ed4-b1383b35aec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# define and print the net\n",
        "net = WordLSTM()\n",
        "print(net)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WordLSTM(\n",
            "  (emb_layer): Embedding(6502, 200)\n",
            "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=6502, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vnhvl6twn0y"
      },
      "source": [
        "# function to generate batches\n",
        "def get_batches(arr_x, arr_y, batch_size):\n",
        "  # iterate through the arrays\n",
        "  prv = 0\n",
        "  \n",
        "  for n in range(batch_size, arr_x.shape[0], batch_size):\n",
        "    # batch of input sequences\n",
        "    x = arr_x[prv:n,:]\n",
        "\n",
        "    # batch of target sequences\n",
        "    y = arr_y[prv:n,:]\n",
        "\n",
        "    prv = n\n",
        "    \n",
        "    yield x, y"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWxMd1nUw-Fr"
      },
      "source": [
        "## 5.2 Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsKDEBiIwn8m"
      },
      "source": [
        "def train(net, epochs=10, batch_size=32, lr=0.001, print_every=32):\n",
        "      \n",
        "  # set initial loss to infinite\n",
        "  best_valid_loss = float('inf')\n",
        "  \n",
        "  # optimizer\n",
        "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  \n",
        "  # loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  \n",
        "  if(torch.cuda.is_available()):\n",
        "    # push model to GPU\n",
        "    net.cuda()\n",
        "  \n",
        "  counter = 0\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  for e in range(epochs):\n",
        "            \n",
        "\n",
        "    # iterate over batches\n",
        "    for x, y in get_batches(x_tr_int, y_tr_int, batch_size):\n",
        "      counter+= 1\n",
        "      \n",
        "      # convert arrays to tensors\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "      \n",
        "      if(torch.cuda.is_available()):\n",
        "        # push tensors to GPU\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      # initialize hidden state\n",
        "      h = net.init_hidden(batch_size)\n",
        "\n",
        "      # set accumulated gradients to zero\n",
        "      net.zero_grad()\n",
        "      \n",
        "      # get the output from the model\n",
        "      output, h = net(inputs, h)\n",
        "      \n",
        "      # calculate the loss and perform backprop\n",
        "      loss = criterion(output, targets.view(-1))\n",
        "      loss.backward()\n",
        "      \n",
        "      opt.step()\n",
        "      \n",
        "      if counter % print_every == 0:\n",
        "        # Get validation loss\n",
        "        \n",
        "        val_losses = []\n",
        "\n",
        "        net.eval()\n",
        "        for x, y in get_batches(x_val_int, y_val_int, batch_size):\n",
        "            \n",
        "          x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "          \n",
        "          val_h = net.init_hidden(batch_size)\n",
        "\n",
        "          inputs, targets = x, y\n",
        "          if(torch.cuda.is_available()):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "          output, val_h = net(inputs, val_h)\n",
        "\n",
        "          val_loss = criterion(output, targets.view(-1))\n",
        "          val_losses.append(val_loss.item())\n",
        "\n",
        "        #save the best model\n",
        "        if np.mean(val_losses) < best_valid_loss:\n",
        "          best_valid_loss = np.mean(val_losses)\n",
        "          torch.save(net.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "        net.train()\n",
        "\n",
        "      \n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "              \"Step: {}...\".format(counter),\n",
        "              \"Loss: {:.4f}...\".format(loss.item()),\n",
        "              \"ppl: {:.4f} \".format(np.exp(np.mean(val_losses))),\n",
        "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FUTLpadwn_c",
        "outputId": "096f4a2e-59ee-411f-e776-843ab37dec4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# specify batch size\n",
        "batch_size = 64\n",
        "\n",
        "# train the model\n",
        "train(net, batch_size = batch_size, epochs=10)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 32... Loss: 6.4096... ppl: 680.7236  Val Loss: 6.5232\n",
            "Epoch: 1/10... Step: 64... Loss: 5.6440... ppl: 347.4264  Val Loss: 5.8506\n",
            "Epoch: 1/10... Step: 96... Loss: 6.1197... ppl: 276.0584  Val Loss: 5.6206\n",
            "Epoch: 1/10... Step: 128... Loss: 5.2403... ppl: 238.9889  Val Loss: 5.4764\n",
            "Epoch: 1/10... Step: 160... Loss: 6.1740... ppl: 211.5311  Val Loss: 5.3544\n",
            "Epoch: 1/10... Step: 192... Loss: 5.7260... ppl: 189.5964  Val Loss: 5.2449\n",
            "Epoch: 1/10... Step: 224... Loss: 5.1836... ppl: 173.3316  Val Loss: 5.1552\n",
            "Epoch: 1/10... Step: 256... Loss: 4.2934... ppl: 158.6628  Val Loss: 5.0668\n",
            "Epoch: 1/10... Step: 288... Loss: 4.8033... ppl: 146.0294  Val Loss: 4.9838\n",
            "Epoch: 1/10... Step: 320... Loss: 4.9913... ppl: 137.0375  Val Loss: 4.9203\n",
            "Epoch: 1/10... Step: 352... Loss: 4.4046... ppl: 127.5059  Val Loss: 4.8482\n",
            "Epoch: 1/10... Step: 384... Loss: 4.9918... ppl: 118.6439  Val Loss: 4.7761\n",
            "Epoch: 1/10... Step: 416... Loss: 4.9670... ppl: 113.2319  Val Loss: 4.7294\n",
            "Epoch: 1/10... Step: 448... Loss: 4.4559... ppl: 108.2055  Val Loss: 4.6840\n",
            "Epoch: 1/10... Step: 480... Loss: 4.3505... ppl: 103.7065  Val Loss: 4.6416\n",
            "Epoch: 1/10... Step: 512... Loss: 4.5931... ppl: 99.5392  Val Loss: 4.6006\n",
            "Epoch: 1/10... Step: 544... Loss: 4.8122... ppl: 95.4082  Val Loss: 4.5582\n",
            "Epoch: 1/10... Step: 576... Loss: 5.5800... ppl: 92.3209  Val Loss: 4.5253\n",
            "Epoch: 1/10... Step: 608... Loss: 4.5703... ppl: 90.2692  Val Loss: 4.5028\n",
            "Epoch: 1/10... Step: 640... Loss: 4.5586... ppl: 86.5122  Val Loss: 4.4603\n",
            "Epoch: 1/10... Step: 672... Loss: 4.4451... ppl: 84.3106  Val Loss: 4.4345\n",
            "Epoch: 1/10... Step: 704... Loss: 4.3097... ppl: 81.7623  Val Loss: 4.4038\n",
            "Epoch: 1/10... Step: 736... Loss: 4.7786... ppl: 80.3492  Val Loss: 4.3864\n",
            "Epoch: 1/10... Step: 768... Loss: 3.9050... ppl: 77.6675  Val Loss: 4.3524\n",
            "Epoch: 1/10... Step: 800... Loss: 4.1742... ppl: 75.9950  Val Loss: 4.3307\n",
            "Epoch: 1/10... Step: 832... Loss: 4.7008... ppl: 74.5126  Val Loss: 4.3110\n",
            "Epoch: 1/10... Step: 864... Loss: 4.1445... ppl: 72.9837  Val Loss: 4.2902\n",
            "Epoch: 1/10... Step: 896... Loss: 4.3766... ppl: 71.7650  Val Loss: 4.2734\n",
            "Epoch: 1/10... Step: 928... Loss: 4.3635... ppl: 69.7373  Val Loss: 4.2447\n",
            "Epoch: 1/10... Step: 960... Loss: 4.0074... ppl: 69.3473  Val Loss: 4.2391\n",
            "Epoch: 1/10... Step: 992... Loss: 3.7500... ppl: 67.2788  Val Loss: 4.2088\n",
            "Epoch: 1/10... Step: 1024... Loss: 3.6645... ppl: 66.1712  Val Loss: 4.1922\n",
            "Epoch: 1/10... Step: 1056... Loss: 4.1369... ppl: 64.8812  Val Loss: 4.1726\n",
            "Epoch: 1/10... Step: 1088... Loss: 3.4901... ppl: 63.8801  Val Loss: 4.1570\n",
            "Epoch: 1/10... Step: 1120... Loss: 4.1322... ppl: 62.8908  Val Loss: 4.1414\n",
            "Epoch: 1/10... Step: 1152... Loss: 3.2095... ppl: 62.1665  Val Loss: 4.1298\n",
            "Epoch: 1/10... Step: 1184... Loss: 3.9790... ppl: 61.2394  Val Loss: 4.1148\n",
            "Epoch: 1/10... Step: 1216... Loss: 3.7926... ppl: 60.5041  Val Loss: 4.1027\n",
            "Epoch: 1/10... Step: 1248... Loss: 4.1518... ppl: 59.3054  Val Loss: 4.0827\n",
            "Epoch: 1/10... Step: 1280... Loss: 4.2154... ppl: 58.4250  Val Loss: 4.0677\n",
            "Epoch: 1/10... Step: 1312... Loss: 4.8066... ppl: 58.4599  Val Loss: 4.0683\n",
            "Epoch: 1/10... Step: 1344... Loss: 3.8476... ppl: 57.5744  Val Loss: 4.0531\n",
            "Epoch: 1/10... Step: 1376... Loss: 4.2301... ppl: 56.5976  Val Loss: 4.0360\n",
            "Epoch: 1/10... Step: 1408... Loss: 3.6366... ppl: 55.8900  Val Loss: 4.0234\n",
            "Epoch: 1/10... Step: 1440... Loss: 4.2292... ppl: 55.5244  Val Loss: 4.0168\n",
            "Epoch: 1/10... Step: 1472... Loss: 3.4389... ppl: 54.8475  Val Loss: 4.0046\n",
            "Epoch: 1/10... Step: 1504... Loss: 4.1207... ppl: 54.6364  Val Loss: 4.0007\n",
            "Epoch: 1/10... Step: 1536... Loss: 4.8280... ppl: 54.3171  Val Loss: 3.9948\n",
            "Epoch: 1/10... Step: 1568... Loss: 3.9407... ppl: 53.2696  Val Loss: 3.9754\n",
            "Epoch: 1/10... Step: 1600... Loss: 3.7166... ppl: 52.7051  Val Loss: 3.9647\n",
            "Epoch: 1/10... Step: 1632... Loss: 5.1706... ppl: 51.9372  Val Loss: 3.9500\n",
            "Epoch: 1/10... Step: 1664... Loss: 4.0039... ppl: 51.4909  Val Loss: 3.9414\n",
            "Epoch: 1/10... Step: 1696... Loss: 3.6618... ppl: 51.0109  Val Loss: 3.9320\n",
            "Epoch: 1/10... Step: 1728... Loss: 3.6808... ppl: 50.6960  Val Loss: 3.9258\n",
            "Epoch: 1/10... Step: 1760... Loss: 4.4876... ppl: 50.5202  Val Loss: 3.9224\n",
            "Epoch: 1/10... Step: 1792... Loss: 3.4509... ppl: 49.9328  Val Loss: 3.9107\n",
            "Epoch: 1/10... Step: 1824... Loss: 4.4149... ppl: 50.2246  Val Loss: 3.9165\n",
            "Epoch: 1/10... Step: 1856... Loss: 5.6877... ppl: 49.0252  Val Loss: 3.8923\n",
            "Epoch: 1/10... Step: 1888... Loss: 3.6751... ppl: 48.4818  Val Loss: 3.8812\n",
            "Epoch: 1/10... Step: 1920... Loss: 3.4624... ppl: 47.9574  Val Loss: 3.8703\n",
            "Epoch: 1/10... Step: 1952... Loss: 3.0435... ppl: 47.7790  Val Loss: 3.8666\n",
            "Epoch: 1/10... Step: 1984... Loss: 2.9341... ppl: 47.7002  Val Loss: 3.8649\n",
            "Epoch: 1/10... Step: 2016... Loss: 3.5936... ppl: 47.1555  Val Loss: 3.8535\n",
            "Epoch: 1/10... Step: 2048... Loss: 4.0417... ppl: 46.8612  Val Loss: 3.8472\n",
            "Epoch: 1/10... Step: 2080... Loss: 3.7994... ppl: 46.6192  Val Loss: 3.8420\n",
            "Epoch: 1/10... Step: 2112... Loss: 4.7229... ppl: 46.5959  Val Loss: 3.8415\n",
            "Epoch: 1/10... Step: 2144... Loss: 3.4828... ppl: 46.0799  Val Loss: 3.8304\n",
            "Epoch: 1/10... Step: 2176... Loss: 5.0702... ppl: 45.8395  Val Loss: 3.8251\n",
            "Epoch: 1/10... Step: 2208... Loss: 4.1689... ppl: 45.7916  Val Loss: 3.8241\n",
            "Epoch: 1/10... Step: 2240... Loss: 3.6272... ppl: 45.6429  Val Loss: 3.8208\n",
            "Epoch: 1/10... Step: 2272... Loss: 3.9198... ppl: 44.8670  Val Loss: 3.8037\n",
            "Epoch: 1/10... Step: 2304... Loss: 3.1970... ppl: 44.5340  Val Loss: 3.7963\n",
            "Epoch: 1/10... Step: 2336... Loss: 4.2683... ppl: 44.3377  Val Loss: 3.7918\n",
            "Epoch: 2/10... Step: 2368... Loss: 3.2237... ppl: 44.3318  Val Loss: 3.7917\n",
            "Epoch: 2/10... Step: 2400... Loss: 3.3797... ppl: 44.2937  Val Loss: 3.7908\n",
            "Epoch: 2/10... Step: 2432... Loss: 3.8194... ppl: 44.4298  Val Loss: 3.7939\n",
            "Epoch: 2/10... Step: 2464... Loss: 4.1496... ppl: 44.1074  Val Loss: 3.7866\n",
            "Epoch: 2/10... Step: 2496... Loss: 4.0050... ppl: 43.6837  Val Loss: 3.7770\n",
            "Epoch: 2/10... Step: 2528... Loss: 3.5167... ppl: 43.4862  Val Loss: 3.7724\n",
            "Epoch: 2/10... Step: 2560... Loss: 5.1184... ppl: 43.1937  Val Loss: 3.7657\n",
            "Epoch: 2/10... Step: 2592... Loss: 4.0394... ppl: 43.4815  Val Loss: 3.7723\n",
            "Epoch: 2/10... Step: 2624... Loss: 3.1700... ppl: 43.1103  Val Loss: 3.7638\n",
            "Epoch: 2/10... Step: 2656... Loss: 3.8260... ppl: 42.8972  Val Loss: 3.7588\n",
            "Epoch: 2/10... Step: 2688... Loss: 3.3853... ppl: 42.5880  Val Loss: 3.7516\n",
            "Epoch: 2/10... Step: 2720... Loss: 3.7952... ppl: 42.1295  Val Loss: 3.7407\n",
            "Epoch: 2/10... Step: 2752... Loss: 3.6660... ppl: 42.3610  Val Loss: 3.7462\n",
            "Epoch: 2/10... Step: 2784... Loss: 4.1798... ppl: 42.3044  Val Loss: 3.7449\n",
            "Epoch: 2/10... Step: 2816... Loss: 4.1017... ppl: 42.1768  Val Loss: 3.7419\n",
            "Epoch: 2/10... Step: 2848... Loss: 3.5549... ppl: 41.7356  Val Loss: 3.7314\n",
            "Epoch: 2/10... Step: 2880... Loss: 4.1271... ppl: 41.5510  Val Loss: 3.7269\n",
            "Epoch: 2/10... Step: 2912... Loss: 3.2342... ppl: 41.2648  Val Loss: 3.7200\n",
            "Epoch: 2/10... Step: 2944... Loss: 3.3608... ppl: 41.4216  Val Loss: 3.7238\n",
            "Epoch: 2/10... Step: 2976... Loss: 3.3189... ppl: 41.1820  Val Loss: 3.7180\n",
            "Epoch: 2/10... Step: 3008... Loss: 4.0691... ppl: 41.1319  Val Loss: 3.7168\n",
            "Epoch: 2/10... Step: 3040... Loss: 3.5195... ppl: 41.0077  Val Loss: 3.7138\n",
            "Epoch: 2/10... Step: 3072... Loss: 3.6615... ppl: 40.7639  Val Loss: 3.7078\n",
            "Epoch: 2/10... Step: 3104... Loss: 3.7621... ppl: 40.5611  Val Loss: 3.7028\n",
            "Epoch: 2/10... Step: 3136... Loss: 4.0816... ppl: 40.4396  Val Loss: 3.6998\n",
            "Epoch: 2/10... Step: 3168... Loss: 3.4111... ppl: 40.3892  Val Loss: 3.6986\n",
            "Epoch: 2/10... Step: 3200... Loss: 3.7601... ppl: 40.4666  Val Loss: 3.7005\n",
            "Epoch: 2/10... Step: 3232... Loss: 4.2329... ppl: 40.1206  Val Loss: 3.6919\n",
            "Epoch: 2/10... Step: 3264... Loss: 2.8755... ppl: 39.7786  Val Loss: 3.6833\n",
            "Epoch: 2/10... Step: 3296... Loss: 4.3051... ppl: 39.9377  Val Loss: 3.6873\n",
            "Epoch: 2/10... Step: 3328... Loss: 4.0615... ppl: 39.6730  Val Loss: 3.6807\n",
            "Epoch: 2/10... Step: 3360... Loss: 3.6622... ppl: 39.3946  Val Loss: 3.6736\n",
            "Epoch: 2/10... Step: 3392... Loss: 3.2203... ppl: 39.5352  Val Loss: 3.6772\n",
            "Epoch: 2/10... Step: 3424... Loss: 4.2157... ppl: 39.3111  Val Loss: 3.6715\n",
            "Epoch: 2/10... Step: 3456... Loss: 3.5307... ppl: 39.0661  Val Loss: 3.6653\n",
            "Epoch: 2/10... Step: 3488... Loss: 3.6509... ppl: 39.2554  Val Loss: 3.6701\n",
            "Epoch: 2/10... Step: 3520... Loss: 3.3860... ppl: 38.9145  Val Loss: 3.6614\n",
            "Epoch: 2/10... Step: 3552... Loss: 4.0992... ppl: 38.8420  Val Loss: 3.6595\n",
            "Epoch: 2/10... Step: 3584... Loss: 2.9522... ppl: 38.6559  Val Loss: 3.6547\n",
            "Epoch: 2/10... Step: 3616... Loss: 3.5083... ppl: 38.5306  Val Loss: 3.6515\n",
            "Epoch: 2/10... Step: 3648... Loss: 3.1948... ppl: 38.6615  Val Loss: 3.6548\n",
            "Epoch: 2/10... Step: 3680... Loss: 3.7120... ppl: 38.5089  Val Loss: 3.6509\n",
            "Epoch: 2/10... Step: 3712... Loss: 3.5115... ppl: 38.3886  Val Loss: 3.6478\n",
            "Epoch: 2/10... Step: 3744... Loss: 3.4050... ppl: 38.0934  Val Loss: 3.6400\n",
            "Epoch: 2/10... Step: 3776... Loss: 4.0522... ppl: 38.2286  Val Loss: 3.6436\n",
            "Epoch: 2/10... Step: 3808... Loss: 5.0302... ppl: 38.0261  Val Loss: 3.6383\n",
            "Epoch: 2/10... Step: 3840... Loss: 4.3982... ppl: 38.3579  Val Loss: 3.6470\n",
            "Epoch: 2/10... Step: 3872... Loss: 4.0262... ppl: 38.0617  Val Loss: 3.6392\n",
            "Epoch: 2/10... Step: 3904... Loss: 2.9649... ppl: 37.8423  Val Loss: 3.6334\n",
            "Epoch: 2/10... Step: 3936... Loss: 3.8122... ppl: 37.9352  Val Loss: 3.6359\n",
            "Epoch: 2/10... Step: 3968... Loss: 4.0267... ppl: 37.5593  Val Loss: 3.6259\n",
            "Epoch: 2/10... Step: 4000... Loss: 3.7960... ppl: 37.4651  Val Loss: 3.6234\n",
            "Epoch: 2/10... Step: 4032... Loss: 4.2121... ppl: 37.4041  Val Loss: 3.6218\n",
            "Epoch: 2/10... Step: 4064... Loss: 3.7036... ppl: 37.3908  Val Loss: 3.6214\n",
            "Epoch: 2/10... Step: 4096... Loss: 3.6282... ppl: 37.4272  Val Loss: 3.6224\n",
            "Epoch: 2/10... Step: 4128... Loss: 2.9239... ppl: 37.2791  Val Loss: 3.6184\n",
            "Epoch: 2/10... Step: 4160... Loss: 2.9132... ppl: 37.3913  Val Loss: 3.6214\n",
            "Epoch: 2/10... Step: 4192... Loss: 3.4737... ppl: 37.1238  Val Loss: 3.6143\n",
            "Epoch: 2/10... Step: 4224... Loss: 3.3310... ppl: 36.7922  Val Loss: 3.6053\n",
            "Epoch: 2/10... Step: 4256... Loss: 3.4062... ppl: 36.8767  Val Loss: 3.6076\n",
            "Epoch: 2/10... Step: 4288... Loss: 3.4935... ppl: 36.7109  Val Loss: 3.6031\n",
            "Epoch: 2/10... Step: 4320... Loss: 2.8813... ppl: 36.7196  Val Loss: 3.6033\n",
            "Epoch: 2/10... Step: 4352... Loss: 3.0807... ppl: 36.6730  Val Loss: 3.6020\n",
            "Epoch: 2/10... Step: 4384... Loss: 3.1373... ppl: 36.5560  Val Loss: 3.5988\n",
            "Epoch: 2/10... Step: 4416... Loss: 3.8817... ppl: 36.5550  Val Loss: 3.5988\n",
            "Epoch: 2/10... Step: 4448... Loss: 2.9740... ppl: 36.7664  Val Loss: 3.6046\n",
            "Epoch: 2/10... Step: 4480... Loss: 4.4027... ppl: 36.5290  Val Loss: 3.5981\n",
            "Epoch: 2/10... Step: 4512... Loss: 3.3350... ppl: 36.3792  Val Loss: 3.5940\n",
            "Epoch: 2/10... Step: 4544... Loss: 3.5276... ppl: 36.3193  Val Loss: 3.5923\n",
            "Epoch: 2/10... Step: 4576... Loss: 2.7399... ppl: 36.3509  Val Loss: 3.5932\n",
            "Epoch: 2/10... Step: 4608... Loss: 2.9132... ppl: 36.3896  Val Loss: 3.5943\n",
            "Epoch: 2/10... Step: 4640... Loss: 3.7600... ppl: 36.1354  Val Loss: 3.5873\n",
            "Epoch: 2/10... Step: 4672... Loss: 3.5162... ppl: 35.9935  Val Loss: 3.5833\n",
            "Epoch: 3/10... Step: 4704... Loss: 3.0776... ppl: 35.9513  Val Loss: 3.5822\n",
            "Epoch: 3/10... Step: 4736... Loss: 4.2246... ppl: 36.2391  Val Loss: 3.5901\n",
            "Epoch: 3/10... Step: 4768... Loss: 3.0695... ppl: 36.1538  Val Loss: 3.5878\n",
            "Epoch: 3/10... Step: 4800... Loss: 2.6759... ppl: 36.1696  Val Loss: 3.5882\n",
            "Epoch: 3/10... Step: 4832... Loss: 3.3219... ppl: 36.0124  Val Loss: 3.5839\n",
            "Epoch: 3/10... Step: 4864... Loss: 3.7528... ppl: 35.8891  Val Loss: 3.5804\n",
            "Epoch: 3/10... Step: 4896... Loss: 2.5550... ppl: 35.9582  Val Loss: 3.5824\n",
            "Epoch: 3/10... Step: 4928... Loss: 2.1217... ppl: 36.0550  Val Loss: 3.5850\n",
            "Epoch: 3/10... Step: 4960... Loss: 3.2081... ppl: 36.0368  Val Loss: 3.5845\n",
            "Epoch: 3/10... Step: 4992... Loss: 3.4374... ppl: 36.2002  Val Loss: 3.5891\n",
            "Epoch: 3/10... Step: 5024... Loss: 3.3935... ppl: 35.8651  Val Loss: 3.5798\n",
            "Epoch: 3/10... Step: 5056... Loss: 3.1415... ppl: 35.6322  Val Loss: 3.5733\n",
            "Epoch: 3/10... Step: 5088... Loss: 2.7343... ppl: 35.6405  Val Loss: 3.5735\n",
            "Epoch: 3/10... Step: 5120... Loss: 3.7148... ppl: 35.8402  Val Loss: 3.5791\n",
            "Epoch: 3/10... Step: 5152... Loss: 2.8669... ppl: 35.9821  Val Loss: 3.5830\n",
            "Epoch: 3/10... Step: 5184... Loss: 2.8398... ppl: 35.8629  Val Loss: 3.5797\n",
            "Epoch: 3/10... Step: 5216... Loss: 3.0145... ppl: 35.4213  Val Loss: 3.5673\n",
            "Epoch: 3/10... Step: 5248... Loss: 3.0084... ppl: 35.3953  Val Loss: 3.5666\n",
            "Epoch: 3/10... Step: 5280... Loss: 3.2460... ppl: 35.6438  Val Loss: 3.5736\n",
            "Epoch: 3/10... Step: 5312... Loss: 3.5066... ppl: 35.5415  Val Loss: 3.5707\n",
            "Epoch: 3/10... Step: 5344... Loss: 3.7750... ppl: 35.4382  Val Loss: 3.5678\n",
            "Epoch: 3/10... Step: 5376... Loss: 4.0780... ppl: 35.4998  Val Loss: 3.5695\n",
            "Epoch: 3/10... Step: 5408... Loss: 3.6996... ppl: 35.4645  Val Loss: 3.5685\n",
            "Epoch: 3/10... Step: 5440... Loss: 4.4319... ppl: 35.4480  Val Loss: 3.5681\n",
            "Epoch: 3/10... Step: 5472... Loss: 3.7555... ppl: 35.1258  Val Loss: 3.5589\n",
            "Epoch: 3/10... Step: 5504... Loss: 3.4849... ppl: 35.3311  Val Loss: 3.5648\n",
            "Epoch: 3/10... Step: 5536... Loss: 3.1895... ppl: 35.5885  Val Loss: 3.5720\n",
            "Epoch: 3/10... Step: 5568... Loss: 3.0311... ppl: 35.1835  Val Loss: 3.5606\n",
            "Epoch: 3/10... Step: 5600... Loss: 3.4214... ppl: 35.1638  Val Loss: 3.5600\n",
            "Epoch: 3/10... Step: 5632... Loss: 3.3681... ppl: 35.1758  Val Loss: 3.5604\n",
            "Epoch: 3/10... Step: 5664... Loss: 3.2159... ppl: 35.0884  Val Loss: 3.5579\n",
            "Epoch: 3/10... Step: 5696... Loss: 3.8020... ppl: 34.9854  Val Loss: 3.5549\n",
            "Epoch: 3/10... Step: 5728... Loss: 3.2135... ppl: 35.2253  Val Loss: 3.5618\n",
            "Epoch: 3/10... Step: 5760... Loss: 3.7323... ppl: 34.9894  Val Loss: 3.5550\n",
            "Epoch: 3/10... Step: 5792... Loss: 3.9276... ppl: 34.8576  Val Loss: 3.5513\n",
            "Epoch: 3/10... Step: 5824... Loss: 3.2049... ppl: 34.9436  Val Loss: 3.5537\n",
            "Epoch: 3/10... Step: 5856... Loss: 3.4209... ppl: 34.6709  Val Loss: 3.5459\n",
            "Epoch: 3/10... Step: 5888... Loss: 3.2067... ppl: 34.6326  Val Loss: 3.5448\n",
            "Epoch: 3/10... Step: 5920... Loss: 3.5740... ppl: 34.5851  Val Loss: 3.5434\n",
            "Epoch: 3/10... Step: 5952... Loss: 3.8581... ppl: 34.5867  Val Loss: 3.5435\n",
            "Epoch: 3/10... Step: 5984... Loss: 3.2093... ppl: 34.6701  Val Loss: 3.5459\n",
            "Epoch: 3/10... Step: 6016... Loss: 3.8859... ppl: 34.5409  Val Loss: 3.5421\n",
            "Epoch: 3/10... Step: 6048... Loss: 3.4410... ppl: 34.7089  Val Loss: 3.5470\n",
            "Epoch: 3/10... Step: 6080... Loss: 2.7192... ppl: 34.3373  Val Loss: 3.5362\n",
            "Epoch: 3/10... Step: 6112... Loss: 3.3123... ppl: 34.3785  Val Loss: 3.5374\n",
            "Epoch: 3/10... Step: 6144... Loss: 3.1359... ppl: 34.3081  Val Loss: 3.5354\n",
            "Epoch: 3/10... Step: 6176... Loss: 3.1837... ppl: 34.3887  Val Loss: 3.5377\n",
            "Epoch: 3/10... Step: 6208... Loss: 3.4706... ppl: 34.4039  Val Loss: 3.5382\n",
            "Epoch: 3/10... Step: 6240... Loss: 2.7580... ppl: 34.3998  Val Loss: 3.5381\n",
            "Epoch: 3/10... Step: 6272... Loss: 2.7293... ppl: 34.5958  Val Loss: 3.5437\n",
            "Epoch: 3/10... Step: 6304... Loss: 3.1944... ppl: 34.4021  Val Loss: 3.5381\n",
            "Epoch: 3/10... Step: 6336... Loss: 3.4528... ppl: 34.2498  Val Loss: 3.5337\n",
            "Epoch: 3/10... Step: 6368... Loss: 3.1375... ppl: 34.2046  Val Loss: 3.5324\n",
            "Epoch: 3/10... Step: 6400... Loss: 3.7469... ppl: 34.2250  Val Loss: 3.5330\n",
            "Epoch: 3/10... Step: 6432... Loss: 3.6021... ppl: 34.1952  Val Loss: 3.5321\n",
            "Epoch: 3/10... Step: 6464... Loss: 3.6663... ppl: 34.2876  Val Loss: 3.5348\n",
            "Epoch: 3/10... Step: 6496... Loss: 2.8401... ppl: 34.1237  Val Loss: 3.5300\n",
            "Epoch: 3/10... Step: 6528... Loss: 3.0299... ppl: 34.2100  Val Loss: 3.5325\n",
            "Epoch: 3/10... Step: 6560... Loss: 3.3205... ppl: 34.0133  Val Loss: 3.5268\n",
            "Epoch: 3/10... Step: 6592... Loss: 2.5755... ppl: 34.1225  Val Loss: 3.5300\n",
            "Epoch: 3/10... Step: 6624... Loss: 3.6468... ppl: 33.9330  Val Loss: 3.5244\n",
            "Epoch: 3/10... Step: 6656... Loss: 2.9398... ppl: 33.8659  Val Loss: 3.5224\n",
            "Epoch: 3/10... Step: 6688... Loss: 2.7232... ppl: 34.0822  Val Loss: 3.5288\n",
            "Epoch: 3/10... Step: 6720... Loss: 2.5161... ppl: 33.9197  Val Loss: 3.5240\n",
            "Epoch: 3/10... Step: 6752... Loss: 3.5674... ppl: 33.8749  Val Loss: 3.5227\n",
            "Epoch: 3/10... Step: 6784... Loss: 3.1561... ppl: 34.0969  Val Loss: 3.5292\n",
            "Epoch: 3/10... Step: 6816... Loss: 2.8675... ppl: 34.1174  Val Loss: 3.5298\n",
            "Epoch: 3/10... Step: 6848... Loss: 2.8238... ppl: 33.7971  Val Loss: 3.5204\n",
            "Epoch: 3/10... Step: 6880... Loss: 3.0690... ppl: 33.9982  Val Loss: 3.5263\n",
            "Epoch: 3/10... Step: 6912... Loss: 4.2156... ppl: 34.0422  Val Loss: 3.5276\n",
            "Epoch: 3/10... Step: 6944... Loss: 3.1984... ppl: 34.2676  Val Loss: 3.5342\n",
            "Epoch: 3/10... Step: 6976... Loss: 3.4260... ppl: 33.9010  Val Loss: 3.5234\n",
            "Epoch: 3/10... Step: 7008... Loss: 3.3664... ppl: 33.7874  Val Loss: 3.5201\n",
            "Epoch: 4/10... Step: 7040... Loss: 3.6063... ppl: 33.6944  Val Loss: 3.5173\n",
            "Epoch: 4/10... Step: 7072... Loss: 3.0186... ppl: 33.9836  Val Loss: 3.5259\n",
            "Epoch: 4/10... Step: 7104... Loss: 3.3153... ppl: 33.7696  Val Loss: 3.5196\n",
            "Epoch: 4/10... Step: 7136... Loss: 2.9014... ppl: 33.9711  Val Loss: 3.5255\n",
            "Epoch: 4/10... Step: 7168... Loss: 3.3839... ppl: 33.9406  Val Loss: 3.5246\n",
            "Epoch: 4/10... Step: 7200... Loss: 2.7300... ppl: 33.8159  Val Loss: 3.5209\n",
            "Epoch: 4/10... Step: 7232... Loss: 3.0316... ppl: 33.9621  Val Loss: 3.5252\n",
            "Epoch: 4/10... Step: 7264... Loss: 4.2414... ppl: 33.8881  Val Loss: 3.5231\n",
            "Epoch: 4/10... Step: 7296... Loss: 3.6548... ppl: 34.0575  Val Loss: 3.5280\n",
            "Epoch: 4/10... Step: 7328... Loss: 2.5718... ppl: 34.2466  Val Loss: 3.5336\n",
            "Epoch: 4/10... Step: 7360... Loss: 2.8340... ppl: 34.0642  Val Loss: 3.5282\n",
            "Epoch: 4/10... Step: 7392... Loss: 3.1877... ppl: 33.8702  Val Loss: 3.5225\n",
            "Epoch: 4/10... Step: 7424... Loss: 2.7129... ppl: 33.6732  Val Loss: 3.5167\n",
            "Epoch: 4/10... Step: 7456... Loss: 2.7203... ppl: 33.8827  Val Loss: 3.5229\n",
            "Epoch: 4/10... Step: 7488... Loss: 3.5290... ppl: 34.1240  Val Loss: 3.5300\n",
            "Epoch: 4/10... Step: 7520... Loss: 2.0221... ppl: 34.1341  Val Loss: 3.5303\n",
            "Epoch: 4/10... Step: 7552... Loss: 3.2292... ppl: 33.8141  Val Loss: 3.5209\n",
            "Epoch: 4/10... Step: 7584... Loss: 2.6595... ppl: 33.8191  Val Loss: 3.5210\n",
            "Epoch: 4/10... Step: 7616... Loss: 3.0643... ppl: 33.9408  Val Loss: 3.5246\n",
            "Epoch: 4/10... Step: 7648... Loss: 3.3737... ppl: 33.8735  Val Loss: 3.5226\n",
            "Epoch: 4/10... Step: 7680... Loss: 2.8102... ppl: 33.9170  Val Loss: 3.5239\n",
            "Epoch: 4/10... Step: 7712... Loss: 2.7294... ppl: 33.8452  Val Loss: 3.5218\n",
            "Epoch: 4/10... Step: 7744... Loss: 3.3153... ppl: 33.9852  Val Loss: 3.5259\n",
            "Epoch: 4/10... Step: 7776... Loss: 3.7096... ppl: 33.9606  Val Loss: 3.5252\n",
            "Epoch: 4/10... Step: 7808... Loss: 3.4596... ppl: 33.7328  Val Loss: 3.5185\n",
            "Epoch: 4/10... Step: 7840... Loss: 2.2157... ppl: 33.9994  Val Loss: 3.5263\n",
            "Epoch: 4/10... Step: 7872... Loss: 3.7752... ppl: 34.0455  Val Loss: 3.5277\n",
            "Epoch: 4/10... Step: 7904... Loss: 2.2677... ppl: 33.8066  Val Loss: 3.5207\n",
            "Epoch: 4/10... Step: 7936... Loss: 3.1159... ppl: 33.8342  Val Loss: 3.5215\n",
            "Epoch: 4/10... Step: 7968... Loss: 3.7882... ppl: 33.6739  Val Loss: 3.5167\n",
            "Epoch: 4/10... Step: 8000... Loss: 2.9714... ppl: 33.8184  Val Loss: 3.5210\n",
            "Epoch: 4/10... Step: 8032... Loss: 3.2899... ppl: 33.8069  Val Loss: 3.5207\n",
            "Epoch: 4/10... Step: 8064... Loss: 2.5093... ppl: 34.0364  Val Loss: 3.5274\n",
            "Epoch: 4/10... Step: 8096... Loss: 2.9821... ppl: 33.8508  Val Loss: 3.5220\n",
            "Epoch: 4/10... Step: 8128... Loss: 2.2776... ppl: 33.7766  Val Loss: 3.5198\n",
            "Epoch: 4/10... Step: 8160... Loss: 2.9598... ppl: 33.7610  Val Loss: 3.5193\n",
            "Epoch: 4/10... Step: 8192... Loss: 3.1614... ppl: 33.7170  Val Loss: 3.5180\n",
            "Epoch: 4/10... Step: 8224... Loss: 2.9780... ppl: 33.5141  Val Loss: 3.5120\n",
            "Epoch: 4/10... Step: 8256... Loss: 2.7022... ppl: 33.4605  Val Loss: 3.5104\n",
            "Epoch: 4/10... Step: 8288... Loss: 3.1320... ppl: 33.5064  Val Loss: 3.5117\n",
            "Epoch: 4/10... Step: 8320... Loss: 3.2888... ppl: 33.5331  Val Loss: 3.5125\n",
            "Epoch: 4/10... Step: 8352... Loss: 2.8252... ppl: 33.4704  Val Loss: 3.5107\n",
            "Epoch: 4/10... Step: 8384... Loss: 2.6604... ppl: 33.7862  Val Loss: 3.5201\n",
            "Epoch: 4/10... Step: 8416... Loss: 2.7756... ppl: 33.4078  Val Loss: 3.5088\n",
            "Epoch: 4/10... Step: 8448... Loss: 4.0437... ppl: 33.4706  Val Loss: 3.5107\n",
            "Epoch: 4/10... Step: 8480... Loss: 2.7438... ppl: 33.4728  Val Loss: 3.5107\n",
            "Epoch: 4/10... Step: 8512... Loss: 2.4779... ppl: 33.3920  Val Loss: 3.5083\n",
            "Epoch: 4/10... Step: 8544... Loss: 3.0862... ppl: 33.5083  Val Loss: 3.5118\n",
            "Epoch: 4/10... Step: 8576... Loss: 2.9625... ppl: 33.4439  Val Loss: 3.5099\n",
            "Epoch: 4/10... Step: 8608... Loss: 3.4544... ppl: 33.5621  Val Loss: 3.5134\n",
            "Epoch: 4/10... Step: 8640... Loss: 3.2657... ppl: 33.6136  Val Loss: 3.5149\n",
            "Epoch: 4/10... Step: 8672... Loss: 2.7002... ppl: 33.4375  Val Loss: 3.5097\n",
            "Epoch: 4/10... Step: 8704... Loss: 3.6850... ppl: 33.3738  Val Loss: 3.5078\n",
            "Epoch: 4/10... Step: 8736... Loss: 3.0370... ppl: 33.4128  Val Loss: 3.5089\n",
            "Epoch: 4/10... Step: 8768... Loss: 2.6657... ppl: 33.4858  Val Loss: 3.5111\n",
            "Epoch: 4/10... Step: 8800... Loss: 3.4940... ppl: 33.4753  Val Loss: 3.5108\n",
            "Epoch: 4/10... Step: 8832... Loss: 2.9620... ppl: 33.3619  Val Loss: 3.5074\n",
            "Epoch: 4/10... Step: 8864... Loss: 3.1994... ppl: 33.5321  Val Loss: 3.5125\n",
            "Epoch: 4/10... Step: 8896... Loss: 2.6568... ppl: 33.3282  Val Loss: 3.5064\n",
            "Epoch: 4/10... Step: 8928... Loss: 2.8750... ppl: 33.3958  Val Loss: 3.5084\n",
            "Epoch: 4/10... Step: 8960... Loss: 2.4109... ppl: 33.3007  Val Loss: 3.5056\n",
            "Epoch: 4/10... Step: 8992... Loss: 3.0138... ppl: 33.2548  Val Loss: 3.5042\n",
            "Epoch: 4/10... Step: 9024... Loss: 3.4423... ppl: 33.4957  Val Loss: 3.5114\n",
            "Epoch: 4/10... Step: 9056... Loss: 2.9881... ppl: 33.2973  Val Loss: 3.5055\n",
            "Epoch: 4/10... Step: 9088... Loss: 3.3639... ppl: 33.2693  Val Loss: 3.5046\n",
            "Epoch: 4/10... Step: 9120... Loss: 2.5625... ppl: 33.4651  Val Loss: 3.5105\n",
            "Epoch: 4/10... Step: 9152... Loss: 3.0281... ppl: 33.5383  Val Loss: 3.5127\n",
            "Epoch: 4/10... Step: 9184... Loss: 3.3155... ppl: 33.2949  Val Loss: 3.5054\n",
            "Epoch: 4/10... Step: 9216... Loss: 2.4849... ppl: 33.3890  Val Loss: 3.5082\n",
            "Epoch: 4/10... Step: 9248... Loss: 3.6871... ppl: 33.4133  Val Loss: 3.5090\n",
            "Epoch: 4/10... Step: 9280... Loss: 2.7186... ppl: 33.9426  Val Loss: 3.5247\n",
            "Epoch: 4/10... Step: 9312... Loss: 3.3824... ppl: 33.4483  Val Loss: 3.5100\n",
            "Epoch: 4/10... Step: 9344... Loss: 3.2881... ppl: 33.2437  Val Loss: 3.5039\n",
            "Epoch: 5/10... Step: 9376... Loss: 1.9782... ppl: 33.2371  Val Loss: 3.5037\n",
            "Epoch: 5/10... Step: 9408... Loss: 3.2063... ppl: 33.4264  Val Loss: 3.5093\n",
            "Epoch: 5/10... Step: 9440... Loss: 2.8236... ppl: 33.2025  Val Loss: 3.5026\n",
            "Epoch: 5/10... Step: 9472... Loss: 2.7866... ppl: 33.3799  Val Loss: 3.5080\n",
            "Epoch: 5/10... Step: 9504... Loss: 3.0780... ppl: 33.4281  Val Loss: 3.5094\n",
            "Epoch: 5/10... Step: 9536... Loss: 2.1582... ppl: 33.3940  Val Loss: 3.5084\n",
            "Epoch: 5/10... Step: 9568... Loss: 1.8347... ppl: 33.5306  Val Loss: 3.5125\n",
            "Epoch: 5/10... Step: 9600... Loss: 3.0584... ppl: 33.4264  Val Loss: 3.5093\n",
            "Epoch: 5/10... Step: 9632... Loss: 2.9775... ppl: 33.6122  Val Loss: 3.5149\n",
            "Epoch: 5/10... Step: 9664... Loss: 3.2505... ppl: 33.6225  Val Loss: 3.5152\n",
            "Epoch: 5/10... Step: 9696... Loss: 2.2538... ppl: 33.7898  Val Loss: 3.5202\n",
            "Epoch: 5/10... Step: 9728... Loss: 3.2361... ppl: 33.5622  Val Loss: 3.5134\n",
            "Epoch: 5/10... Step: 9760... Loss: 2.4068... ppl: 33.2187  Val Loss: 3.5031\n",
            "Epoch: 5/10... Step: 9792... Loss: 3.4420... ppl: 33.4493  Val Loss: 3.5100\n",
            "Epoch: 5/10... Step: 9824... Loss: 3.2479... ppl: 33.7081  Val Loss: 3.5177\n",
            "Epoch: 5/10... Step: 9856... Loss: 2.0579... ppl: 33.8347  Val Loss: 3.5215\n",
            "Epoch: 5/10... Step: 9888... Loss: 3.2062... ppl: 33.5607  Val Loss: 3.5134\n",
            "Epoch: 5/10... Step: 9920... Loss: 2.8908... ppl: 33.6472  Val Loss: 3.5159\n",
            "Epoch: 5/10... Step: 9952... Loss: 3.2023... ppl: 33.5700  Val Loss: 3.5136\n",
            "Epoch: 5/10... Step: 9984... Loss: 3.1479... ppl: 33.6354  Val Loss: 3.5156\n",
            "Epoch: 5/10... Step: 10016... Loss: 2.8076... ppl: 33.7511  Val Loss: 3.5190\n",
            "Epoch: 5/10... Step: 10048... Loss: 2.7599... ppl: 33.6231  Val Loss: 3.5152\n",
            "Epoch: 5/10... Step: 10080... Loss: 2.8802... ppl: 33.6362  Val Loss: 3.5156\n",
            "Epoch: 5/10... Step: 10112... Loss: 3.8069... ppl: 33.8018  Val Loss: 3.5205\n",
            "Epoch: 5/10... Step: 10144... Loss: 2.7164... ppl: 33.5044  Val Loss: 3.5117\n",
            "Epoch: 5/10... Step: 10176... Loss: 2.4746... ppl: 33.7650  Val Loss: 3.5194\n",
            "Epoch: 5/10... Step: 10208... Loss: 3.2741... ppl: 33.8350  Val Loss: 3.5215\n",
            "Epoch: 5/10... Step: 10240... Loss: 3.1074... ppl: 33.7145  Val Loss: 3.5179\n",
            "Epoch: 5/10... Step: 10272... Loss: 3.6833... ppl: 33.6129  Val Loss: 3.5149\n",
            "Epoch: 5/10... Step: 10304... Loss: 3.6004... ppl: 33.4526  Val Loss: 3.5101\n",
            "Epoch: 5/10... Step: 10336... Loss: 2.9774... ppl: 33.6119  Val Loss: 3.5149\n",
            "Epoch: 5/10... Step: 10368... Loss: 2.8474... ppl: 33.6293  Val Loss: 3.5154\n",
            "Epoch: 5/10... Step: 10400... Loss: 2.9733... ppl: 33.6379  Val Loss: 3.5157\n",
            "Epoch: 5/10... Step: 10432... Loss: 2.8839... ppl: 33.8662  Val Loss: 3.5224\n",
            "Epoch: 5/10... Step: 10464... Loss: 2.9904... ppl: 33.7090  Val Loss: 3.5178\n",
            "Epoch: 5/10... Step: 10496... Loss: 2.5703... ppl: 33.7229  Val Loss: 3.5182\n",
            "Epoch: 5/10... Step: 10528... Loss: 3.2895... ppl: 33.7798  Val Loss: 3.5199\n",
            "Epoch: 5/10... Step: 10560... Loss: 3.5346... ppl: 33.5101  Val Loss: 3.5118\n",
            "Epoch: 5/10... Step: 10592... Loss: 2.8783... ppl: 33.4648  Val Loss: 3.5105\n",
            "Epoch: 5/10... Step: 10624... Loss: 2.9289... ppl: 33.5246  Val Loss: 3.5123\n",
            "Epoch: 5/10... Step: 10656... Loss: 2.9581... ppl: 33.5014  Val Loss: 3.5116\n",
            "Epoch: 5/10... Step: 10688... Loss: 2.8523... ppl: 33.5517  Val Loss: 3.5131\n",
            "Epoch: 5/10... Step: 10720... Loss: 2.8805... ppl: 33.7835  Val Loss: 3.5200\n",
            "Epoch: 5/10... Step: 10752... Loss: 2.6587... ppl: 33.5899  Val Loss: 3.5142\n",
            "Epoch: 5/10... Step: 10784... Loss: 3.5749... ppl: 33.4779  Val Loss: 3.5109\n",
            "Epoch: 5/10... Step: 10816... Loss: 3.1361... ppl: 33.7019  Val Loss: 3.5176\n",
            "Epoch: 5/10... Step: 10848... Loss: 3.3463... ppl: 33.4783  Val Loss: 3.5109\n",
            "Epoch: 5/10... Step: 10880... Loss: 2.8280... ppl: 33.5252  Val Loss: 3.5123\n",
            "Epoch: 5/10... Step: 10912... Loss: 2.4459... ppl: 33.4873  Val Loss: 3.5112\n",
            "Epoch: 5/10... Step: 10944... Loss: 3.1163... ppl: 33.4729  Val Loss: 3.5107\n",
            "Epoch: 5/10... Step: 10976... Loss: 3.1545... ppl: 33.6259  Val Loss: 3.5153\n",
            "Epoch: 5/10... Step: 11008... Loss: 3.1677... ppl: 33.5935  Val Loss: 3.5143\n",
            "Epoch: 5/10... Step: 11040... Loss: 3.6891... ppl: 33.5260  Val Loss: 3.5123\n",
            "Epoch: 5/10... Step: 11072... Loss: 3.4586... ppl: 33.6133  Val Loss: 3.5149\n",
            "Epoch: 5/10... Step: 11104... Loss: 3.0558... ppl: 33.5887  Val Loss: 3.5142\n",
            "Epoch: 5/10... Step: 11136... Loss: 3.5240... ppl: 33.5387  Val Loss: 3.5127\n",
            "Epoch: 5/10... Step: 11168... Loss: 2.4006... ppl: 33.6031  Val Loss: 3.5146\n",
            "Epoch: 5/10... Step: 11200... Loss: 2.8471... ppl: 33.7204  Val Loss: 3.5181\n",
            "Epoch: 5/10... Step: 11232... Loss: 2.7026... ppl: 33.4853  Val Loss: 3.5111\n",
            "Epoch: 5/10... Step: 11264... Loss: 2.1777... ppl: 33.4991  Val Loss: 3.5115\n",
            "Epoch: 5/10... Step: 11296... Loss: 2.6665... ppl: 33.6003  Val Loss: 3.5145\n",
            "Epoch: 5/10... Step: 11328... Loss: 3.5918... ppl: 33.4023  Val Loss: 3.5086\n",
            "Epoch: 5/10... Step: 11360... Loss: 2.8043... ppl: 33.7004  Val Loss: 3.5175\n",
            "Epoch: 5/10... Step: 11392... Loss: 2.5725... ppl: 33.5416  Val Loss: 3.5128\n",
            "Epoch: 5/10... Step: 11424... Loss: 3.3796... ppl: 33.5307  Val Loss: 3.5125\n",
            "Epoch: 5/10... Step: 11456... Loss: 2.2493... ppl: 33.6444  Val Loss: 3.5158\n",
            "Epoch: 5/10... Step: 11488... Loss: 2.8050... ppl: 33.7142  Val Loss: 3.5179\n",
            "Epoch: 5/10... Step: 11520... Loss: 2.2849... ppl: 33.5880  Val Loss: 3.5142\n",
            "Epoch: 5/10... Step: 11552... Loss: 3.7169... ppl: 33.4786  Val Loss: 3.5109\n",
            "Epoch: 5/10... Step: 11584... Loss: 2.9094... ppl: 33.5674  Val Loss: 3.5136\n",
            "Epoch: 5/10... Step: 11616... Loss: 2.3934... ppl: 34.0188  Val Loss: 3.5269\n",
            "Epoch: 5/10... Step: 11648... Loss: 2.8106... ppl: 33.8832  Val Loss: 3.5229\n",
            "Epoch: 5/10... Step: 11680... Loss: 3.2028... ppl: 33.4722  Val Loss: 3.5107\n",
            "Epoch: 5/10... Step: 11712... Loss: 3.1264... ppl: 33.4524  Val Loss: 3.5101\n",
            "Epoch: 6/10... Step: 11744... Loss: 3.4405... ppl: 33.6176  Val Loss: 3.5150\n",
            "Epoch: 6/10... Step: 11776... Loss: 2.8097... ppl: 33.5278  Val Loss: 3.5124\n",
            "Epoch: 6/10... Step: 11808... Loss: 2.6291... ppl: 33.4663  Val Loss: 3.5105\n",
            "Epoch: 6/10... Step: 11840... Loss: 3.3026... ppl: 33.5986  Val Loss: 3.5145\n",
            "Epoch: 6/10... Step: 11872... Loss: 2.8373... ppl: 33.6190  Val Loss: 3.5151\n",
            "Epoch: 6/10... Step: 11904... Loss: 2.9703... ppl: 33.7108  Val Loss: 3.5178\n",
            "Epoch: 6/10... Step: 11936... Loss: 2.8306... ppl: 33.6750  Val Loss: 3.5168\n",
            "Epoch: 6/10... Step: 11968... Loss: 2.6692... ppl: 33.7867  Val Loss: 3.5201\n",
            "Epoch: 6/10... Step: 12000... Loss: 2.5236... ppl: 33.7987  Val Loss: 3.5204\n",
            "Epoch: 6/10... Step: 12032... Loss: 2.9827... ppl: 34.1942  Val Loss: 3.5321\n",
            "Epoch: 6/10... Step: 12064... Loss: 3.0664... ppl: 33.8795  Val Loss: 3.5228\n",
            "Epoch: 6/10... Step: 12096... Loss: 3.8187... ppl: 33.6469  Val Loss: 3.5159\n",
            "Epoch: 6/10... Step: 12128... Loss: 2.5185... ppl: 33.7234  Val Loss: 3.5182\n",
            "Epoch: 6/10... Step: 12160... Loss: 2.9719... ppl: 34.0683  Val Loss: 3.5284\n",
            "Epoch: 6/10... Step: 12192... Loss: 2.9000... ppl: 34.2215  Val Loss: 3.5329\n",
            "Epoch: 6/10... Step: 12224... Loss: 2.6537... ppl: 34.0537  Val Loss: 3.5279\n",
            "Epoch: 6/10... Step: 12256... Loss: 2.6276... ppl: 34.0136  Val Loss: 3.5268\n",
            "Epoch: 6/10... Step: 12288... Loss: 2.7942... ppl: 33.9423  Val Loss: 3.5247\n",
            "Epoch: 6/10... Step: 12320... Loss: 2.8830... ppl: 34.0775  Val Loss: 3.5286\n",
            "Epoch: 6/10... Step: 12352... Loss: 2.6695... ppl: 34.1300  Val Loss: 3.5302\n",
            "Epoch: 6/10... Step: 12384... Loss: 3.0817... ppl: 34.0115  Val Loss: 3.5267\n",
            "Epoch: 6/10... Step: 12416... Loss: 2.9581... ppl: 34.1145  Val Loss: 3.5297\n",
            "Epoch: 6/10... Step: 12448... Loss: 2.8890... ppl: 34.0936  Val Loss: 3.5291\n",
            "Epoch: 6/10... Step: 12480... Loss: 2.5760... ppl: 34.0424  Val Loss: 3.5276\n",
            "Epoch: 6/10... Step: 12512... Loss: 2.8482... ppl: 34.0142  Val Loss: 3.5268\n",
            "Epoch: 6/10... Step: 12544... Loss: 2.9226... ppl: 34.2180  Val Loss: 3.5328\n",
            "Epoch: 6/10... Step: 12576... Loss: 2.4064... ppl: 34.4233  Val Loss: 3.5387\n",
            "Epoch: 6/10... Step: 12608... Loss: 3.1939... ppl: 34.1730  Val Loss: 3.5314\n",
            "Epoch: 6/10... Step: 12640... Loss: 2.6591... ppl: 33.9380  Val Loss: 3.5245\n",
            "Epoch: 6/10... Step: 12672... Loss: 3.1122... ppl: 34.1534  Val Loss: 3.5309\n",
            "Epoch: 6/10... Step: 12704... Loss: 2.5267... ppl: 34.0947  Val Loss: 3.5291\n",
            "Epoch: 6/10... Step: 12736... Loss: 3.2070... ppl: 34.0375  Val Loss: 3.5275\n",
            "Epoch: 6/10... Step: 12768... Loss: 2.6365... ppl: 34.4638  Val Loss: 3.5399\n",
            "Epoch: 6/10... Step: 12800... Loss: 2.8946... ppl: 34.2214  Val Loss: 3.5329\n",
            "Epoch: 6/10... Step: 12832... Loss: 2.9423... ppl: 34.2305  Val Loss: 3.5331\n",
            "Epoch: 6/10... Step: 12864... Loss: 2.9020... ppl: 34.2294  Val Loss: 3.5331\n",
            "Epoch: 6/10... Step: 12896... Loss: 3.2844... ppl: 33.9325  Val Loss: 3.5244\n",
            "Epoch: 6/10... Step: 12928... Loss: 2.6824... ppl: 33.9273  Val Loss: 3.5242\n",
            "Epoch: 6/10... Step: 12960... Loss: 2.9496... ppl: 33.9812  Val Loss: 3.5258\n",
            "Epoch: 6/10... Step: 12992... Loss: 2.9856... ppl: 34.0462  Val Loss: 3.5277\n",
            "Epoch: 6/10... Step: 13024... Loss: 2.4068... ppl: 34.1412  Val Loss: 3.5305\n",
            "Epoch: 6/10... Step: 13056... Loss: 2.8210... ppl: 34.2372  Val Loss: 3.5333\n",
            "Epoch: 6/10... Step: 13088... Loss: 2.7169... ppl: 34.3327  Val Loss: 3.5361\n",
            "Epoch: 6/10... Step: 13120... Loss: 3.2236... ppl: 34.0218  Val Loss: 3.5270\n",
            "Epoch: 6/10... Step: 13152... Loss: 2.8444... ppl: 34.2567  Val Loss: 3.5339\n",
            "Epoch: 6/10... Step: 13184... Loss: 3.2372... ppl: 34.0461  Val Loss: 3.5277\n",
            "Epoch: 6/10... Step: 13216... Loss: 3.4146... ppl: 34.0227  Val Loss: 3.5270\n",
            "Epoch: 6/10... Step: 13248... Loss: 3.3366... ppl: 34.0046  Val Loss: 3.5265\n",
            "Epoch: 6/10... Step: 13280... Loss: 3.2357... ppl: 34.0580  Val Loss: 3.5281\n",
            "Epoch: 6/10... Step: 13312... Loss: 2.8473... ppl: 34.1200  Val Loss: 3.5299\n",
            "Epoch: 6/10... Step: 13344... Loss: 2.4289... ppl: 34.2458  Val Loss: 3.5336\n",
            "Epoch: 6/10... Step: 13376... Loss: 2.5387... ppl: 34.1550  Val Loss: 3.5309\n",
            "Epoch: 6/10... Step: 13408... Loss: 2.1943... ppl: 34.2176  Val Loss: 3.5327\n",
            "Epoch: 6/10... Step: 13440... Loss: 3.2943... ppl: 34.1739  Val Loss: 3.5315\n",
            "Epoch: 6/10... Step: 13472... Loss: 3.1102... ppl: 34.0859  Val Loss: 3.5289\n",
            "Epoch: 6/10... Step: 13504... Loss: 2.8960... ppl: 34.2782  Val Loss: 3.5345\n",
            "Epoch: 6/10... Step: 13536... Loss: 3.2225... ppl: 34.2233  Val Loss: 3.5329\n",
            "Epoch: 6/10... Step: 13568... Loss: 2.8418... ppl: 34.0848  Val Loss: 3.5289\n",
            "Epoch: 6/10... Step: 13600... Loss: 2.7739... ppl: 33.9933  Val Loss: 3.5262\n",
            "Epoch: 6/10... Step: 13632... Loss: 3.6860... ppl: 34.2860  Val Loss: 3.5347\n",
            "Epoch: 6/10... Step: 13664... Loss: 2.7332... ppl: 34.0275  Val Loss: 3.5272\n",
            "Epoch: 6/10... Step: 13696... Loss: 3.2151... ppl: 34.1228  Val Loss: 3.5300\n",
            "Epoch: 6/10... Step: 13728... Loss: 2.4705... ppl: 34.2918  Val Loss: 3.5349\n",
            "Epoch: 6/10... Step: 13760... Loss: 3.2255... ppl: 34.1009  Val Loss: 3.5293\n",
            "Epoch: 6/10... Step: 13792... Loss: 3.0137... ppl: 34.1424  Val Loss: 3.5305\n",
            "Epoch: 6/10... Step: 13824... Loss: 2.5673... ppl: 34.4060  Val Loss: 3.5382\n",
            "Epoch: 6/10... Step: 13856... Loss: 3.2379... ppl: 34.3353  Val Loss: 3.5362\n",
            "Epoch: 6/10... Step: 13888... Loss: 2.8395... ppl: 34.0908  Val Loss: 3.5290\n",
            "Epoch: 6/10... Step: 13920... Loss: 3.3830... ppl: 34.1720  Val Loss: 3.5314\n",
            "Epoch: 6/10... Step: 13952... Loss: 2.2182... ppl: 34.4393  Val Loss: 3.5392\n",
            "Epoch: 6/10... Step: 13984... Loss: 3.0430... ppl: 34.6158  Val Loss: 3.5443\n",
            "Epoch: 6/10... Step: 14016... Loss: 2.5535... ppl: 34.1977  Val Loss: 3.5322\n",
            "Epoch: 6/10... Step: 14048... Loss: 2.4920... ppl: 34.1616  Val Loss: 3.5311\n",
            "Epoch: 7/10... Step: 14080... Loss: 3.0443... ppl: 34.2283  Val Loss: 3.5331\n",
            "Epoch: 7/10... Step: 14112... Loss: 2.0756... ppl: 34.3089  Val Loss: 3.5354\n",
            "Epoch: 7/10... Step: 14144... Loss: 2.7825... ppl: 34.0683  Val Loss: 3.5284\n",
            "Epoch: 7/10... Step: 14176... Loss: 2.6419... ppl: 34.2230  Val Loss: 3.5329\n",
            "Epoch: 7/10... Step: 14208... Loss: 2.4645... ppl: 34.3393  Val Loss: 3.5363\n",
            "Epoch: 7/10... Step: 14240... Loss: 2.7937... ppl: 34.3211  Val Loss: 3.5358\n",
            "Epoch: 7/10... Step: 14272... Loss: 2.4278... ppl: 34.3404  Val Loss: 3.5363\n",
            "Epoch: 7/10... Step: 14304... Loss: 2.3435... ppl: 34.5075  Val Loss: 3.5412\n",
            "Epoch: 7/10... Step: 14336... Loss: 2.6369... ppl: 34.4256  Val Loss: 3.5388\n",
            "Epoch: 7/10... Step: 14368... Loss: 3.4982... ppl: 34.9423  Val Loss: 3.5537\n",
            "Epoch: 7/10... Step: 14400... Loss: 2.5662... ppl: 34.6195  Val Loss: 3.5444\n",
            "Epoch: 7/10... Step: 14432... Loss: 2.9304... ppl: 34.4406  Val Loss: 3.5392\n",
            "Epoch: 7/10... Step: 14464... Loss: 3.0882... ppl: 34.2783  Val Loss: 3.5345\n",
            "Epoch: 7/10... Step: 14496... Loss: 3.1150... ppl: 34.7146  Val Loss: 3.5472\n",
            "Epoch: 7/10... Step: 14528... Loss: 2.1675... ppl: 34.9310  Val Loss: 3.5534\n",
            "Epoch: 7/10... Step: 14560... Loss: 2.4720... ppl: 34.7893  Val Loss: 3.5493\n",
            "Epoch: 7/10... Step: 14592... Loss: 2.5484... ppl: 34.6400  Val Loss: 3.5450\n",
            "Epoch: 7/10... Step: 14624... Loss: 2.1780... ppl: 34.6645  Val Loss: 3.5457\n",
            "Epoch: 7/10... Step: 14656... Loss: 2.9759... ppl: 34.8783  Val Loss: 3.5519\n",
            "Epoch: 7/10... Step: 14688... Loss: 3.0350... ppl: 34.8736  Val Loss: 3.5517\n",
            "Epoch: 7/10... Step: 14720... Loss: 3.7475... ppl: 34.8784  Val Loss: 3.5519\n",
            "Epoch: 7/10... Step: 14752... Loss: 3.0685... ppl: 34.8746  Val Loss: 3.5518\n",
            "Epoch: 7/10... Step: 14784... Loss: 2.1849... ppl: 34.8467  Val Loss: 3.5510\n",
            "Epoch: 7/10... Step: 14816... Loss: 2.7164... ppl: 34.9399  Val Loss: 3.5536\n",
            "Epoch: 7/10... Step: 14848... Loss: 2.7426... ppl: 34.6647  Val Loss: 3.5457\n",
            "Epoch: 7/10... Step: 14880... Loss: 2.7844... ppl: 34.9961  Val Loss: 3.5552\n",
            "Epoch: 7/10... Step: 14912... Loss: 2.6043... ppl: 35.2517  Val Loss: 3.5625\n",
            "Epoch: 7/10... Step: 14944... Loss: 3.1207... ppl: 34.9543  Val Loss: 3.5540\n",
            "Epoch: 7/10... Step: 14976... Loss: 2.6044... ppl: 34.7034  Val Loss: 3.5468\n",
            "Epoch: 7/10... Step: 15008... Loss: 2.9243... ppl: 34.8948  Val Loss: 3.5523\n",
            "Epoch: 7/10... Step: 15040... Loss: 2.1260... ppl: 34.8910  Val Loss: 3.5522\n",
            "Epoch: 7/10... Step: 15072... Loss: 3.0618... ppl: 34.8248  Val Loss: 3.5503\n",
            "Epoch: 7/10... Step: 15104... Loss: 2.8557... ppl: 35.2771  Val Loss: 3.5632\n",
            "Epoch: 7/10... Step: 15136... Loss: 3.0239... ppl: 35.1106  Val Loss: 3.5585\n",
            "Epoch: 7/10... Step: 15168... Loss: 2.3165... ppl: 35.0097  Val Loss: 3.5556\n",
            "Epoch: 7/10... Step: 15200... Loss: 2.6745... ppl: 35.0208  Val Loss: 3.5559\n",
            "Epoch: 7/10... Step: 15232... Loss: 3.1288... ppl: 34.7887  Val Loss: 3.5493\n",
            "Epoch: 7/10... Step: 15264... Loss: 2.7829... ppl: 34.7797  Val Loss: 3.5490\n",
            "Epoch: 7/10... Step: 15296... Loss: 2.9868... ppl: 34.7346  Val Loss: 3.5477\n",
            "Epoch: 7/10... Step: 15328... Loss: 2.6283... ppl: 34.6294  Val Loss: 3.5447\n",
            "Epoch: 7/10... Step: 15360... Loss: 2.7738... ppl: 34.8391  Val Loss: 3.5507\n",
            "Epoch: 7/10... Step: 15392... Loss: 2.7348... ppl: 34.8877  Val Loss: 3.5521\n",
            "Epoch: 7/10... Step: 15424... Loss: 2.3094... ppl: 35.1843  Val Loss: 3.5606\n",
            "Epoch: 7/10... Step: 15456... Loss: 3.0400... ppl: 34.8499  Val Loss: 3.5511\n",
            "Epoch: 7/10... Step: 15488... Loss: 2.7198... ppl: 34.9011  Val Loss: 3.5525\n",
            "Epoch: 7/10... Step: 15520... Loss: 2.9265... ppl: 34.8799  Val Loss: 3.5519\n",
            "Epoch: 7/10... Step: 15552... Loss: 2.6978... ppl: 34.6907  Val Loss: 3.5465\n",
            "Epoch: 7/10... Step: 15584... Loss: 3.3223... ppl: 34.7634  Val Loss: 3.5486\n",
            "Epoch: 7/10... Step: 15616... Loss: 2.0960... ppl: 34.7621  Val Loss: 3.5485\n",
            "Epoch: 7/10... Step: 15648... Loss: 2.8494... ppl: 34.8478  Val Loss: 3.5510\n",
            "Epoch: 7/10... Step: 15680... Loss: 2.6631... ppl: 35.0128  Val Loss: 3.5557\n",
            "Epoch: 7/10... Step: 15712... Loss: 2.2027... ppl: 34.9498  Val Loss: 3.5539\n",
            "Epoch: 7/10... Step: 15744... Loss: 2.8312... ppl: 35.1139  Val Loss: 3.5586\n",
            "Epoch: 7/10... Step: 15776... Loss: 2.9157... ppl: 34.9743  Val Loss: 3.5546\n",
            "Epoch: 7/10... Step: 15808... Loss: 3.0818... ppl: 35.0513  Val Loss: 3.5568\n",
            "Epoch: 7/10... Step: 15840... Loss: 3.5904... ppl: 35.0487  Val Loss: 3.5567\n",
            "Epoch: 7/10... Step: 15872... Loss: 3.2785... ppl: 34.8987  Val Loss: 3.5524\n",
            "Epoch: 7/10... Step: 15904... Loss: 2.4712... ppl: 34.9901  Val Loss: 3.5551\n",
            "Epoch: 7/10... Step: 15936... Loss: 2.6874... ppl: 34.8025  Val Loss: 3.5497\n",
            "Epoch: 7/10... Step: 15968... Loss: 1.9452... ppl: 35.1234  Val Loss: 3.5589\n",
            "Epoch: 7/10... Step: 16000... Loss: 2.8524... ppl: 34.9231  Val Loss: 3.5531\n",
            "Epoch: 7/10... Step: 16032... Loss: 2.4998... ppl: 34.8167  Val Loss: 3.5501\n",
            "Epoch: 7/10... Step: 16064... Loss: 2.3388... ppl: 35.1748  Val Loss: 3.5603\n",
            "Epoch: 7/10... Step: 16096... Loss: 2.2038... ppl: 34.9507  Val Loss: 3.5539\n",
            "Epoch: 7/10... Step: 16128... Loss: 2.5278... ppl: 34.8537  Val Loss: 3.5512\n",
            "Epoch: 7/10... Step: 16160... Loss: 2.6790... ppl: 35.1731  Val Loss: 3.5603\n",
            "Epoch: 7/10... Step: 16192... Loss: 2.6223... ppl: 35.1935  Val Loss: 3.5609\n",
            "Epoch: 7/10... Step: 16224... Loss: 3.1063... ppl: 34.8695  Val Loss: 3.5516\n",
            "Epoch: 7/10... Step: 16256... Loss: 2.3833... ppl: 35.0371  Val Loss: 3.5564\n",
            "Epoch: 7/10... Step: 16288... Loss: 2.7428... ppl: 35.1189  Val Loss: 3.5587\n",
            "Epoch: 7/10... Step: 16320... Loss: 2.2094... ppl: 35.5993  Val Loss: 3.5723\n",
            "Epoch: 7/10... Step: 16352... Loss: 2.4741... ppl: 35.3071  Val Loss: 3.5641\n",
            "Epoch: 7/10... Step: 16384... Loss: 3.3849... ppl: 34.9821  Val Loss: 3.5548\n",
            "Epoch: 8/10... Step: 16416... Loss: 3.3633... ppl: 35.0531  Val Loss: 3.5569\n",
            "Epoch: 8/10... Step: 16448... Loss: 2.6553... ppl: 35.2655  Val Loss: 3.5629\n",
            "Epoch: 8/10... Step: 16480... Loss: 2.6547... ppl: 34.8272  Val Loss: 3.5504\n",
            "Epoch: 8/10... Step: 16512... Loss: 1.7663... ppl: 34.9601  Val Loss: 3.5542\n",
            "Epoch: 8/10... Step: 16544... Loss: 2.6592... ppl: 35.1380  Val Loss: 3.5593\n",
            "Epoch: 8/10... Step: 16576... Loss: 2.8427... ppl: 35.1900  Val Loss: 3.5608\n",
            "Epoch: 8/10... Step: 16608... Loss: 2.8938... ppl: 35.2907  Val Loss: 3.5636\n",
            "Epoch: 8/10... Step: 16640... Loss: 2.4102... ppl: 35.2696  Val Loss: 3.5630\n",
            "Epoch: 8/10... Step: 16672... Loss: 2.6647... ppl: 35.3341  Val Loss: 3.5648\n",
            "Epoch: 8/10... Step: 16704... Loss: 2.7308... ppl: 35.8363  Val Loss: 3.5790\n",
            "Epoch: 8/10... Step: 16736... Loss: 3.1749... ppl: 35.7000  Val Loss: 3.5752\n",
            "Epoch: 8/10... Step: 16768... Loss: 2.8225... ppl: 35.4140  Val Loss: 3.5671\n",
            "Epoch: 8/10... Step: 16800... Loss: 2.3086... ppl: 35.1326  Val Loss: 3.5591\n",
            "Epoch: 8/10... Step: 16832... Loss: 2.4003... ppl: 35.4410  Val Loss: 3.5679\n",
            "Epoch: 8/10... Step: 16864... Loss: 2.7034... ppl: 35.7902  Val Loss: 3.5777\n",
            "Epoch: 8/10... Step: 16896... Loss: 2.3340... ppl: 35.7858  Val Loss: 3.5776\n",
            "Epoch: 8/10... Step: 16928... Loss: 2.4512... ppl: 35.5212  Val Loss: 3.5701\n",
            "Epoch: 8/10... Step: 16960... Loss: 2.3321... ppl: 35.6173  Val Loss: 3.5728\n",
            "Epoch: 8/10... Step: 16992... Loss: 2.6894... ppl: 35.7632  Val Loss: 3.5769\n",
            "Epoch: 8/10... Step: 17024... Loss: 2.2406... ppl: 35.7052  Val Loss: 3.5753\n",
            "Epoch: 8/10... Step: 17056... Loss: 2.5663... ppl: 35.7564  Val Loss: 3.5767\n",
            "Epoch: 8/10... Step: 17088... Loss: 3.2127... ppl: 35.6716  Val Loss: 3.5744\n",
            "Epoch: 8/10... Step: 17120... Loss: 2.6826... ppl: 35.8286  Val Loss: 3.5787\n",
            "Epoch: 8/10... Step: 17152... Loss: 3.1223... ppl: 35.9118  Val Loss: 3.5811\n",
            "Epoch: 8/10... Step: 17184... Loss: 2.4881... ppl: 35.5537  Val Loss: 3.5710\n",
            "Epoch: 8/10... Step: 17216... Loss: 2.5009... ppl: 35.8771  Val Loss: 3.5801\n",
            "Epoch: 8/10... Step: 17248... Loss: 2.2832... ppl: 36.0134  Val Loss: 3.5839\n",
            "Epoch: 8/10... Step: 17280... Loss: 2.4110... ppl: 35.9283  Val Loss: 3.5815\n",
            "Epoch: 8/10... Step: 17312... Loss: 3.4089... ppl: 35.6604  Val Loss: 3.5740\n",
            "Epoch: 8/10... Step: 17344... Loss: 2.5734... ppl: 35.8099  Val Loss: 3.5782\n",
            "Epoch: 8/10... Step: 17376... Loss: 2.7577... ppl: 35.8847  Val Loss: 3.5803\n",
            "Epoch: 8/10... Step: 17408... Loss: 2.6888... ppl: 35.8246  Val Loss: 3.5786\n",
            "Epoch: 8/10... Step: 17440... Loss: 2.6891... ppl: 36.0758  Val Loss: 3.5856\n",
            "Epoch: 8/10... Step: 17472... Loss: 2.7555... ppl: 36.0817  Val Loss: 3.5858\n",
            "Epoch: 8/10... Step: 17504... Loss: 2.7711... ppl: 35.8623  Val Loss: 3.5797\n",
            "Epoch: 8/10... Step: 17536... Loss: 2.7681... ppl: 35.9848  Val Loss: 3.5831\n",
            "Epoch: 8/10... Step: 17568... Loss: 2.7584... ppl: 35.7759  Val Loss: 3.5773\n",
            "Epoch: 8/10... Step: 17600... Loss: 2.6833... ppl: 35.6726  Val Loss: 3.5744\n",
            "Epoch: 8/10... Step: 17632... Loss: 2.6452... ppl: 35.5596  Val Loss: 3.5712\n",
            "Epoch: 8/10... Step: 17664... Loss: 2.5788... ppl: 35.5139  Val Loss: 3.5699\n",
            "Epoch: 8/10... Step: 17696... Loss: 3.0945... ppl: 35.5672  Val Loss: 3.5714\n",
            "Epoch: 8/10... Step: 17728... Loss: 3.0820... ppl: 35.6599  Val Loss: 3.5740\n",
            "Epoch: 8/10... Step: 17760... Loss: 2.5773... ppl: 36.0869  Val Loss: 3.5859\n",
            "Epoch: 8/10... Step: 17792... Loss: 2.5090... ppl: 35.8224  Val Loss: 3.5786\n",
            "Epoch: 8/10... Step: 17824... Loss: 2.8284... ppl: 35.7130  Val Loss: 3.5755\n",
            "Epoch: 8/10... Step: 17856... Loss: 2.3179... ppl: 35.8175  Val Loss: 3.5784\n",
            "Epoch: 8/10... Step: 17888... Loss: 2.7714... ppl: 35.6175  Val Loss: 3.5728\n",
            "Epoch: 8/10... Step: 17920... Loss: 2.2119... ppl: 35.6941  Val Loss: 3.5750\n",
            "Epoch: 8/10... Step: 17952... Loss: 2.7615... ppl: 35.6954  Val Loss: 3.5750\n",
            "Epoch: 8/10... Step: 17984... Loss: 2.6187... ppl: 35.7604  Val Loss: 3.5768\n",
            "Epoch: 8/10... Step: 18016... Loss: 2.7388... ppl: 35.8397  Val Loss: 3.5791\n",
            "Epoch: 8/10... Step: 18048... Loss: 3.1212... ppl: 35.8359  Val Loss: 3.5789\n",
            "Epoch: 8/10... Step: 18080... Loss: 3.0363... ppl: 36.0018  Val Loss: 3.5836\n",
            "Epoch: 8/10... Step: 18112... Loss: 2.4549... ppl: 35.9448  Val Loss: 3.5820\n",
            "Epoch: 8/10... Step: 18144... Loss: 3.3395... ppl: 35.8979  Val Loss: 3.5807\n",
            "Epoch: 8/10... Step: 18176... Loss: 2.9733... ppl: 35.9795  Val Loss: 3.5829\n",
            "Epoch: 8/10... Step: 18208... Loss: 2.8302... ppl: 35.9355  Val Loss: 3.5817\n",
            "Epoch: 8/10... Step: 18240... Loss: 2.8140... ppl: 36.0589  Val Loss: 3.5852\n",
            "Epoch: 8/10... Step: 18272... Loss: 2.0532... ppl: 35.8227  Val Loss: 3.5786\n",
            "Epoch: 8/10... Step: 18304... Loss: 2.7801... ppl: 35.9881  Val Loss: 3.5832\n",
            "Epoch: 8/10... Step: 18336... Loss: 3.0013... ppl: 35.8903  Val Loss: 3.5805\n",
            "Epoch: 8/10... Step: 18368... Loss: 2.9666... ppl: 35.6483  Val Loss: 3.5737\n",
            "Epoch: 8/10... Step: 18400... Loss: 2.6893... ppl: 36.1413  Val Loss: 3.5874\n",
            "Epoch: 8/10... Step: 18432... Loss: 3.0349... ppl: 36.0321  Val Loss: 3.5844\n",
            "Epoch: 8/10... Step: 18464... Loss: 2.4117... ppl: 35.9400  Val Loss: 3.5819\n",
            "Epoch: 8/10... Step: 18496... Loss: 2.0754... ppl: 36.1658  Val Loss: 3.5881\n",
            "Epoch: 8/10... Step: 18528... Loss: 2.9170... ppl: 36.3221  Val Loss: 3.5924\n",
            "Epoch: 8/10... Step: 18560... Loss: 2.2853... ppl: 35.9017  Val Loss: 3.5808\n",
            "Epoch: 8/10... Step: 18592... Loss: 2.7173... ppl: 35.8951  Val Loss: 3.5806\n",
            "Epoch: 8/10... Step: 18624... Loss: 2.6806... ppl: 36.0804  Val Loss: 3.5857\n",
            "Epoch: 8/10... Step: 18656... Loss: 3.4551... ppl: 36.6920  Val Loss: 3.6026\n",
            "Epoch: 8/10... Step: 18688... Loss: 2.1651... ppl: 36.4260  Val Loss: 3.5953\n",
            "Epoch: 8/10... Step: 18720... Loss: 2.4987... ppl: 35.9718  Val Loss: 3.5827\n",
            "Epoch: 9/10... Step: 18752... Loss: 2.4598... ppl: 35.9907  Val Loss: 3.5833\n",
            "Epoch: 9/10... Step: 18784... Loss: 2.5598... ppl: 36.2868  Val Loss: 3.5915\n",
            "Epoch: 9/10... Step: 18816... Loss: 2.7622... ppl: 35.8632  Val Loss: 3.5797\n",
            "Epoch: 9/10... Step: 18848... Loss: 2.8331... ppl: 35.8620  Val Loss: 3.5797\n",
            "Epoch: 9/10... Step: 18880... Loss: 2.7895... ppl: 36.1067  Val Loss: 3.5865\n",
            "Epoch: 9/10... Step: 18912... Loss: 2.8334... ppl: 36.1682  Val Loss: 3.5882\n",
            "Epoch: 9/10... Step: 18944... Loss: 2.7383... ppl: 36.3256  Val Loss: 3.5925\n",
            "Epoch: 9/10... Step: 18976... Loss: 2.9009... ppl: 36.2012  Val Loss: 3.5891\n",
            "Epoch: 9/10... Step: 19008... Loss: 2.9640... ppl: 36.3578  Val Loss: 3.5934\n",
            "Epoch: 9/10... Step: 19040... Loss: 2.5248... ppl: 36.4561  Val Loss: 3.5961\n",
            "Epoch: 9/10... Step: 19072... Loss: 2.4702... ppl: 36.8207  Val Loss: 3.6061\n",
            "Epoch: 9/10... Step: 19104... Loss: 2.4660... ppl: 36.5076  Val Loss: 3.5975\n",
            "Epoch: 9/10... Step: 19136... Loss: 1.9129... ppl: 36.1117  Val Loss: 3.5866\n",
            "Epoch: 9/10... Step: 19168... Loss: 2.3594... ppl: 36.2278  Val Loss: 3.5898\n",
            "Epoch: 9/10... Step: 19200... Loss: 3.1214... ppl: 36.7706  Val Loss: 3.6047\n",
            "Epoch: 9/10... Step: 19232... Loss: 2.8790... ppl: 36.8166  Val Loss: 3.6059\n",
            "Epoch: 9/10... Step: 19264... Loss: 2.7350... ppl: 36.4141  Val Loss: 3.5950\n",
            "Epoch: 9/10... Step: 19296... Loss: 3.2109... ppl: 36.5725  Val Loss: 3.5993\n",
            "Epoch: 9/10... Step: 19328... Loss: 2.8662... ppl: 36.6122  Val Loss: 3.6004\n",
            "Epoch: 9/10... Step: 19360... Loss: 2.8863... ppl: 36.6405  Val Loss: 3.6012\n",
            "Epoch: 9/10... Step: 19392... Loss: 2.3970... ppl: 36.9042  Val Loss: 3.6083\n",
            "Epoch: 9/10... Step: 19424... Loss: 2.7365... ppl: 36.6127  Val Loss: 3.6004\n",
            "Epoch: 9/10... Step: 19456... Loss: 2.6789... ppl: 36.8788  Val Loss: 3.6076\n",
            "Epoch: 9/10... Step: 19488... Loss: 2.5296... ppl: 36.9597  Val Loss: 3.6098\n",
            "Epoch: 9/10... Step: 19520... Loss: 2.5601... ppl: 36.6189  Val Loss: 3.6006\n",
            "Epoch: 9/10... Step: 19552... Loss: 2.2552... ppl: 36.7607  Val Loss: 3.6044\n",
            "Epoch: 9/10... Step: 19584... Loss: 2.2964... ppl: 36.8597  Val Loss: 3.6071\n",
            "Epoch: 9/10... Step: 19616... Loss: 2.8798... ppl: 37.0482  Val Loss: 3.6122\n",
            "Epoch: 9/10... Step: 19648... Loss: 2.7019... ppl: 36.7485  Val Loss: 3.6041\n",
            "Epoch: 9/10... Step: 19680... Loss: 2.6840... ppl: 36.6331  Val Loss: 3.6010\n",
            "Epoch: 9/10... Step: 19712... Loss: 2.3154... ppl: 36.8810  Val Loss: 3.6077\n",
            "Epoch: 9/10... Step: 19744... Loss: 1.9490... ppl: 36.8242  Val Loss: 3.6062\n",
            "Epoch: 9/10... Step: 19776... Loss: 2.9817... ppl: 36.9831  Val Loss: 3.6105\n",
            "Epoch: 9/10... Step: 19808... Loss: 1.9941... ppl: 37.1961  Val Loss: 3.6162\n",
            "Epoch: 9/10... Step: 19840... Loss: 2.3150... ppl: 37.0533  Val Loss: 3.6124\n",
            "Epoch: 9/10... Step: 19872... Loss: 2.7944... ppl: 37.0616  Val Loss: 3.6126\n",
            "Epoch: 9/10... Step: 19904... Loss: 2.4667... ppl: 36.8089  Val Loss: 3.6057\n",
            "Epoch: 9/10... Step: 19936... Loss: 2.4165... ppl: 36.5735  Val Loss: 3.5993\n",
            "Epoch: 9/10... Step: 19968... Loss: 2.5493... ppl: 36.5292  Val Loss: 3.5981\n",
            "Epoch: 9/10... Step: 20000... Loss: 3.0519... ppl: 36.5525  Val Loss: 3.5987\n",
            "Epoch: 9/10... Step: 20032... Loss: 2.7701... ppl: 36.4437  Val Loss: 3.5958\n",
            "Epoch: 9/10... Step: 20064... Loss: 2.5180... ppl: 36.4568  Val Loss: 3.5961\n",
            "Epoch: 9/10... Step: 20096... Loss: 2.5483... ppl: 36.7585  Val Loss: 3.6044\n",
            "Epoch: 9/10... Step: 20128... Loss: 2.7052... ppl: 36.9992  Val Loss: 3.6109\n",
            "Epoch: 9/10... Step: 20160... Loss: 2.8687... ppl: 36.7273  Val Loss: 3.6035\n",
            "Epoch: 9/10... Step: 20192... Loss: 2.3282... ppl: 36.9150  Val Loss: 3.6086\n",
            "Epoch: 9/10... Step: 20224... Loss: 2.4433... ppl: 36.7032  Val Loss: 3.6029\n",
            "Epoch: 9/10... Step: 20256... Loss: 2.3298... ppl: 36.5637  Val Loss: 3.5991\n",
            "Epoch: 9/10... Step: 20288... Loss: 2.7331... ppl: 36.6941  Val Loss: 3.6026\n",
            "Epoch: 9/10... Step: 20320... Loss: 2.5729... ppl: 36.7455  Val Loss: 3.6040\n",
            "Epoch: 9/10... Step: 20352... Loss: 2.4402... ppl: 36.8896  Val Loss: 3.6079\n",
            "Epoch: 9/10... Step: 20384... Loss: 2.8826... ppl: 36.9086  Val Loss: 3.6084\n",
            "Epoch: 9/10... Step: 20416... Loss: 2.5934... ppl: 37.0998  Val Loss: 3.6136\n",
            "Epoch: 9/10... Step: 20448... Loss: 2.5307... ppl: 37.0297  Val Loss: 3.6117\n",
            "Epoch: 9/10... Step: 20480... Loss: 2.4876... ppl: 37.0873  Val Loss: 3.6133\n",
            "Epoch: 9/10... Step: 20512... Loss: 2.7434... ppl: 37.0345  Val Loss: 3.6119\n",
            "Epoch: 9/10... Step: 20544... Loss: 2.5824... ppl: 37.0290  Val Loss: 3.6117\n",
            "Epoch: 9/10... Step: 20576... Loss: 2.5496... ppl: 37.0762  Val Loss: 3.6130\n",
            "Epoch: 9/10... Step: 20608... Loss: 2.4823... ppl: 36.7976  Val Loss: 3.6054\n",
            "Epoch: 9/10... Step: 20640... Loss: 2.2978... ppl: 36.9418  Val Loss: 3.6093\n",
            "Epoch: 9/10... Step: 20672... Loss: 2.7142... ppl: 37.0094  Val Loss: 3.6112\n",
            "Epoch: 9/10... Step: 20704... Loss: 2.6697... ppl: 36.7716  Val Loss: 3.6047\n",
            "Epoch: 9/10... Step: 20736... Loss: 2.4588... ppl: 37.0440  Val Loss: 3.6121\n",
            "Epoch: 9/10... Step: 20768... Loss: 2.8723... ppl: 37.0625  Val Loss: 3.6126\n",
            "Epoch: 9/10... Step: 20800... Loss: 2.8864... ppl: 36.8930  Val Loss: 3.6080\n",
            "Epoch: 9/10... Step: 20832... Loss: 2.8252... ppl: 37.0980  Val Loss: 3.6136\n",
            "Epoch: 9/10... Step: 20864... Loss: 2.4235... ppl: 37.3723  Val Loss: 3.6209\n",
            "Epoch: 9/10... Step: 20896... Loss: 2.6108... ppl: 37.1842  Val Loss: 3.6159\n",
            "Epoch: 9/10... Step: 20928... Loss: 2.4546... ppl: 36.9614  Val Loss: 3.6099\n",
            "Epoch: 9/10... Step: 20960... Loss: 2.5937... ppl: 37.0556  Val Loss: 3.6124\n",
            "Epoch: 9/10... Step: 20992... Loss: 2.1802... ppl: 37.6676  Val Loss: 3.6288\n",
            "Epoch: 9/10... Step: 21024... Loss: 2.3488... ppl: 37.6198  Val Loss: 3.6275\n",
            "Epoch: 9/10... Step: 21056... Loss: 2.6070... ppl: 37.1642  Val Loss: 3.6153\n",
            "Epoch: 10/10... Step: 21088... Loss: 2.7194... ppl: 36.9788  Val Loss: 3.6103\n",
            "Epoch: 10/10... Step: 21120... Loss: 2.5199... ppl: 37.3578  Val Loss: 3.6205\n",
            "Epoch: 10/10... Step: 21152... Loss: 2.7200... ppl: 36.8908  Val Loss: 3.6080\n",
            "Epoch: 10/10... Step: 21184... Loss: 2.7523... ppl: 36.8453  Val Loss: 3.6067\n",
            "Epoch: 10/10... Step: 21216... Loss: 2.2953... ppl: 36.9276  Val Loss: 3.6090\n",
            "Epoch: 10/10... Step: 21248... Loss: 2.4539... ppl: 37.1732  Val Loss: 3.6156\n",
            "Epoch: 10/10... Step: 21280... Loss: 3.1250... ppl: 37.4570  Val Loss: 3.6232\n",
            "Epoch: 10/10... Step: 21312... Loss: 2.6855... ppl: 37.2116  Val Loss: 3.6166\n",
            "Epoch: 10/10... Step: 21344... Loss: 2.6734... ppl: 37.5144  Val Loss: 3.6247\n",
            "Epoch: 10/10... Step: 21376... Loss: 2.3815... ppl: 37.4601  Val Loss: 3.6233\n",
            "Epoch: 10/10... Step: 21408... Loss: 2.6202... ppl: 37.9005  Val Loss: 3.6350\n",
            "Epoch: 10/10... Step: 21440... Loss: 2.7384... ppl: 37.6697  Val Loss: 3.6289\n",
            "Epoch: 10/10... Step: 21472... Loss: 2.5399... ppl: 37.2989  Val Loss: 3.6190\n",
            "Epoch: 10/10... Step: 21504... Loss: 2.5387... ppl: 37.1565  Val Loss: 3.6151\n",
            "Epoch: 10/10... Step: 21536... Loss: 2.5052... ppl: 37.6667  Val Loss: 3.6288\n",
            "Epoch: 10/10... Step: 21568... Loss: 2.7164... ppl: 37.9313  Val Loss: 3.6358\n",
            "Epoch: 10/10... Step: 21600... Loss: 2.3104... ppl: 37.7473  Val Loss: 3.6309\n",
            "Epoch: 10/10... Step: 21632... Loss: 2.4243... ppl: 37.8119  Val Loss: 3.6326\n",
            "Epoch: 10/10... Step: 21664... Loss: 2.5260... ppl: 37.6460  Val Loss: 3.6282\n",
            "Epoch: 10/10... Step: 21696... Loss: 3.0087... ppl: 37.8475  Val Loss: 3.6336\n",
            "Epoch: 10/10... Step: 21728... Loss: 2.4794... ppl: 37.8519  Val Loss: 3.6337\n",
            "Epoch: 10/10... Step: 21760... Loss: 2.1673... ppl: 37.7415  Val Loss: 3.6308\n",
            "Epoch: 10/10... Step: 21792... Loss: 2.2418... ppl: 37.9345  Val Loss: 3.6359\n",
            "Epoch: 10/10... Step: 21824... Loss: 2.5471... ppl: 38.0009  Val Loss: 3.6376\n",
            "Epoch: 10/10... Step: 21856... Loss: 2.5276... ppl: 37.9386  Val Loss: 3.6360\n",
            "Epoch: 10/10... Step: 21888... Loss: 2.3883... ppl: 37.8448  Val Loss: 3.6335\n",
            "Epoch: 10/10... Step: 21920... Loss: 2.9707... ppl: 38.0287  Val Loss: 3.6383\n",
            "Epoch: 10/10... Step: 21952... Loss: 2.6753... ppl: 38.1861  Val Loss: 3.6425\n",
            "Epoch: 10/10... Step: 21984... Loss: 1.9349... ppl: 37.9653  Val Loss: 3.6367\n",
            "Epoch: 10/10... Step: 22016... Loss: 2.6746... ppl: 37.6314  Val Loss: 3.6278\n",
            "Epoch: 10/10... Step: 22048... Loss: 2.2032... ppl: 38.0624  Val Loss: 3.6392\n",
            "Epoch: 10/10... Step: 22080... Loss: 2.8172... ppl: 37.9679  Val Loss: 3.6367\n",
            "Epoch: 10/10... Step: 22112... Loss: 2.0454... ppl: 37.8419  Val Loss: 3.6334\n",
            "Epoch: 10/10... Step: 22144... Loss: 2.2547... ppl: 38.2876  Val Loss: 3.6451\n",
            "Epoch: 10/10... Step: 22176... Loss: 2.2573... ppl: 38.1426  Val Loss: 3.6413\n",
            "Epoch: 10/10... Step: 22208... Loss: 2.9155... ppl: 38.1832  Val Loss: 3.6424\n",
            "Epoch: 10/10... Step: 22240... Loss: 2.5912... ppl: 38.1079  Val Loss: 3.6404\n",
            "Epoch: 10/10... Step: 22272... Loss: 2.6798... ppl: 37.6735  Val Loss: 3.6290\n",
            "Epoch: 10/10... Step: 22304... Loss: 2.6931... ppl: 37.6337  Val Loss: 3.6279\n",
            "Epoch: 10/10... Step: 22336... Loss: 2.5015... ppl: 37.6082  Val Loss: 3.6272\n",
            "Epoch: 10/10... Step: 22368... Loss: 3.0899... ppl: 37.4667  Val Loss: 3.6235\n",
            "Epoch: 10/10... Step: 22400... Loss: 2.5177... ppl: 37.5643  Val Loss: 3.6261\n",
            "Epoch: 10/10... Step: 22432... Loss: 2.3608... ppl: 37.7555  Val Loss: 3.6311\n",
            "Epoch: 10/10... Step: 22464... Loss: 1.7979... ppl: 38.1142  Val Loss: 3.6406\n",
            "Epoch: 10/10... Step: 22496... Loss: 2.4616... ppl: 37.8056  Val Loss: 3.6325\n",
            "Epoch: 10/10... Step: 22528... Loss: 3.0315... ppl: 38.1455  Val Loss: 3.6414\n",
            "Epoch: 10/10... Step: 22560... Loss: 2.5864... ppl: 37.9769  Val Loss: 3.6370\n",
            "Epoch: 10/10... Step: 22592... Loss: 2.3030... ppl: 37.6755  Val Loss: 3.6290\n",
            "Epoch: 10/10... Step: 22624... Loss: 2.7715... ppl: 37.6923  Val Loss: 3.6295\n",
            "Epoch: 10/10... Step: 22656... Loss: 2.2744... ppl: 37.9173  Val Loss: 3.6354\n",
            "Epoch: 10/10... Step: 22688... Loss: 2.7655... ppl: 37.9585  Val Loss: 3.6365\n",
            "Epoch: 10/10... Step: 22720... Loss: 2.4769... ppl: 38.1889  Val Loss: 3.6425\n",
            "Epoch: 10/10... Step: 22752... Loss: 2.9444... ppl: 38.2322  Val Loss: 3.6437\n",
            "Epoch: 10/10... Step: 22784... Loss: 2.7382... ppl: 38.3060  Val Loss: 3.6456\n",
            "Epoch: 10/10... Step: 22816... Loss: 2.6001... ppl: 38.2542  Val Loss: 3.6443\n",
            "Epoch: 10/10... Step: 22848... Loss: 2.7592... ppl: 38.1131  Val Loss: 3.6406\n",
            "Epoch: 10/10... Step: 22880... Loss: 2.1229... ppl: 38.2912  Val Loss: 3.6452\n",
            "Epoch: 10/10... Step: 22912... Loss: 1.9738... ppl: 38.3284  Val Loss: 3.6462\n",
            "Epoch: 10/10... Step: 22944... Loss: 2.6351... ppl: 38.0769  Val Loss: 3.6396\n",
            "Epoch: 10/10... Step: 22976... Loss: 2.3999... ppl: 37.9894  Val Loss: 3.6373\n",
            "Epoch: 10/10... Step: 23008... Loss: 2.6892... ppl: 38.2704  Val Loss: 3.6447\n",
            "Epoch: 10/10... Step: 23040... Loss: 2.3490... ppl: 37.9402  Val Loss: 3.6360\n",
            "Epoch: 10/10... Step: 23072... Loss: 2.6154... ppl: 38.0893  Val Loss: 3.6399\n",
            "Epoch: 10/10... Step: 23104... Loss: 2.4611... ppl: 38.2623  Val Loss: 3.6445\n",
            "Epoch: 10/10... Step: 23136... Loss: 2.4236... ppl: 38.1983  Val Loss: 3.6428\n",
            "Epoch: 10/10... Step: 23168... Loss: 2.5879... ppl: 38.1957  Val Loss: 3.6427\n",
            "Epoch: 10/10... Step: 23200... Loss: 2.2508... ppl: 38.4859  Val Loss: 3.6503\n",
            "Epoch: 10/10... Step: 23232... Loss: 2.4229... ppl: 38.4149  Val Loss: 3.6484\n",
            "Epoch: 10/10... Step: 23264... Loss: 2.5469... ppl: 37.9751  Val Loss: 3.6369\n",
            "Epoch: 10/10... Step: 23296... Loss: 2.4882... ppl: 38.0308  Val Loss: 3.6384\n",
            "Epoch: 10/10... Step: 23328... Loss: 2.5016... ppl: 38.4954  Val Loss: 3.6505\n",
            "Epoch: 10/10... Step: 23360... Loss: 2.9927... ppl: 38.7518  Val Loss: 3.6572\n",
            "Epoch: 10/10... Step: 23392... Loss: 2.7558... ppl: 38.3281  Val Loss: 3.6462\n",
            "Epoch: 10/10... Step: 23424... Loss: 2.1572... ppl: 37.9144  Val Loss: 3.6353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GZ7jod75is8"
      },
      "source": [
        "# 6. Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tJ1k7Sb6E1i",
        "outputId": "216c7981-022f-46ba-e247-73ce67cc56b6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-48ba7724-04de-4ae0-83b9-58dcdd2882a1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-48ba7724-04de-4ae0-83b9-58dcdd2882a1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving saved_weights.pt to saved_weights (1).pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiR-5aBs5iUm",
        "outputId": "97c4125b-412f-426c-9e28-f451d6928846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "net.load_state_dict(torch.load(path))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvfT2F6fwoXM"
      },
      "source": [
        "# function to generate one token\n",
        "def predict(net, tkn, h=None):\n",
        "         \n",
        "  # tensor inputs\n",
        "  x = np.array([[token2int[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "  \n",
        "  if(torch.cuda.is_available()):\n",
        "      inputs = inputs.cuda()\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  if(torch.cuda.is_available()):\n",
        "      p = p.cpu()\n",
        "\n",
        "  p = p.numpy()\n",
        "  sampled_token_index = np.argmax(p, axis = 1)[0]\n",
        "  \n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return int2token[sampled_token_index], h"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUdht2S-wn5S"
      },
      "source": [
        "# function to fetch generated sequence\n",
        "def sample(net, size = 2, seed_text='it is'):\n",
        "        \n",
        "    if(torch.cuda.is_available()):\n",
        "        net.cuda()\n",
        "    \n",
        "    net.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    toks = seed_text.split()\n",
        "\n",
        "    # predict next token\n",
        "    for t in toks:\n",
        "      token, h = predict(net, t, h)\n",
        "    \n",
        "    toks.append(token)\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        token, h = predict(net, toks[-1], h)\n",
        "        toks.append(token)\n",
        "\n",
        "    return ' '.join(toks)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsydZz7Kwn37",
        "outputId": "655638ba-4972-4790-c243-60d2c2324478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "# seed texts\n",
        "seeds = [\"i want to\",\n",
        "         \"how about a cup\",\n",
        "         \"i don't want\",\n",
        "         \"can you send\",\n",
        "         \"my car\"]\n",
        "\n",
        "# number of tokens to generate\n",
        "num_toks = 6\n",
        "\n",
        "# text generation\n",
        "for s in seeds:\n",
        "  # get generated text from the model\n",
        "  text_gen = sample(net, num_toks, seed_text=s)\n",
        "  # print the result\n",
        "  print(\"seed text:\", s, \">> output:\",text_gen)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seed text: i want to >> output: i want to order a pizza from pizza hut\n",
            "\n",
            "\n",
            "seed text: how about a cup >> output: how about a cup of pizza with extra sauce and\n",
            "\n",
            "\n",
            "seed text: i don't want >> output: i don't want to get to the <unk> inn\n",
            "\n",
            "\n",
            "seed text: can you send >> output: can you send me the receipt to my phone\n",
            "\n",
            "\n",
            "seed text: my car >> output: my car is making a loud rattle coming\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwr1n3mZwnzg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uyVb9oVwnxO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPAp_kD4tLum"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S4RCtawtLqj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du81YPWOtLoS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFMEhuD3BXqk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSAm6rTyBX-T"
      },
      "source": [
        ""
      ]
    }
  ]
}